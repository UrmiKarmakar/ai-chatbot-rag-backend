{"order": ["2_1", "2_2", "2_3", "2_4", "2_5", "2_6", "2_7", "2_8", "2_9", "2_10", "2_11", "2_12", "2_13", "2_14", "2_15", "2_16", "2_17", "2_18", "2_19", "2_20", "2_21", "2_22", "2_23", "2_24", "2_25", "2_26", "2_27", "2_28", "2_29", "2_30", "2_31", "2_32", "2_33", "2_34", "2_35", "2_36", "2_37", "2_38", "2_39", "2_40", "2_41", "2_42", "2_43", "2_44", "2_45", "2_46", "2_47", "2_48", "2_49", "2_50", "2_51", "2_52", "2_53", "2_54", "2_55", "2_56", "2_57", "2_58", "2_59", "2_60", "2_61", "2_62", "2_63", "2_64", "2_65", "2_66", "2_67", "2_68", "2_69", "2_70", "2_71", "2_72", "2_73", "2_74", "2_75", "2_76", "2_77", "2_78", "2_79", "2_80", "2_81", "2_82", "2_83", "2_84", "2_85", "2_86", "2_87", "2_88", "2_89", "2_90", "2_91", "2_92", "2_93", "2_94", "2_95", "2_96", "2_97", "2_98", "2_99", "2_100", "2_101", "2_102", "2_103", "2_104", "2_105", "2_106", "2_107", "2_108", "2_109", "2_110", "2_111", "2_112", "2_113", "2_114", "2_115", "2_116", "2_117", "2_118", "2_119", "2_120", "2_121", "2_122", "2_123", "2_124", "2_125", "2_126", "2_127", "2_128", "2_129", "2_130", "2_131", "2_132", "2_133", "2_134", "2_135", "2_136", "2_137", "2_138", "2_139", "2_140", "2_141", "2_142", "2_143", "2_144", "2_145", "2_146", "2_147", "2_148", "2_149", "2_150", "2_151", "2_152", "2_153", "2_154", "2_155", "2_156", "2_157", "2_158", "2_159", "2_160", "2_161", "2_162", "2_163", "2_164", "2_165", "2_166", "2_167", "2_168", "2_169", "2_170", "2_171", "2_172", "2_173", "2_174", "2_175", "2_176", "2_177", "2_178", "2_179", "2_180", "2_181", "2_182", "2_183", "2_184", "2_185", "2_186", "2_187", "2_188", "2_189", "2_190", "2_191", "2_192", "2_193", "2_194", "2_195", "2_196", "2_197", "2_198", "2_199", "2_200", "2_201", "2_202", "2_203", "2_204", "2_205", "2_206", "2_207", "2_208", "2_209", "2_210", "2_211", "2_212", "2_213", "2_214", "2_215", "2_216", "2_217", "2_218", "2_219", "2_220", "2_221", "2_222", "2_223", "2_224", "2_225", "2_226", "2_227", "2_228", "2_229", "2_230", "2_231", "2_232", "2_233", "2_234", "2_235", "2_236", "2_237", "2_238", "2_239", "2_240", "2_241", "2_242", "2_243", "2_244", "2_245", "2_246", "2_247", "2_248", "2_249", "2_250", "2_251", "2_252", "2_253", "2_254", "2_255", "2_256", "2_257", "2_258", "2_259", "2_260", "2_261", "2_262"], "docs": {"2_1": {"id": "2_1", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ReactGenie: A Development Framework for Complex Multimodal \nInteractions Using Large Language Models \nJackie Junrui Yang Yingtian Shi Yuhan Zhang \njackiey@stanford.edu shiyt0313@gmail.com zhangyh@stanford.edu \nStanford University Tsinghua University Stanford University \nStanford, CA, USA Beijing, China Stanford, CA, USA \nKarina Li Daniel Wan Rosli Anisha Jain \nkarinali@stanford.edu danwr@stanford.edu anishaj037@gmail.com \nStanford University Stanford University Independent Researcher \nStanford, ", "source": "database"}, "2_2": {"id": "2_2", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "CA, USA Stanford, CA, USA USA \nShuning Zhang Tianshi Li James A. Landay \nzhang-sn19@mails.tsinghua.edu.cn tia.li@northeastern.edu landay@stanford.edu \nTsinghua University Northeastern University Stanford University \nBeijing, China Boston, MA, USA Stanford, CA, USA \nMonica S. Lam \nlam@cs.stanford.edu \nStanford University \nStanford, CA, USA \nABSTRACT \nBy combining voice and touch interactions, multimodal interfaces \ncan surpass the efficiency of either modality alone. Traditional mul-\ntimodal fram", "source": "database"}, "2_3": {"id": "2_3", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "eworks require laborious developer work to support \nrich multimodal commands where the user’s multimodal command \ninvolves possibly exponential combinations of actions/function \ninvocations. This paper presents ReactGenie, a programming frame-\nwork that better separates multimodal input from the computational \nmodel to enable developers to create efficient and capable multi-\nmodal interfaces with ease. ReactGenie translates multimodal user \ncommands into NLPL (Natural Language Programming Langua", "source": "database"}, "2_4": {"id": "2_4", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ge), \na programming language we created, using a neural semantic parser \nbased on large-language models. The ReactGenie runtime interprets \nthe parsed NLPL and composes primitives in the computational \nmodel to implement complex user commands. As a result, React-\nGenie allows easy implementation and unprecedented richness \nin commands for end-users of multimodal apps. Our evaluation \nshowed that 12 developers can learn and build a non-trivial Re-\nactGenie application in under 2.5 hours on averag", "source": "database"}, "2_5": {"id": "2_5", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e. In addition, \ncompared with a traditional GUI, end-users can complete tasks \nfaster and with less task load using ReactGenie apps. \nThis work is licensed under a Creative Commons Attribution-Share Alike \nInternational 4.0 License. \nCHI ’24, May 11–16, 2024, Honolulu, HI, USA \n© 2024 Copyright held by the owner/author(s). \nACM ISBN 979-8-4007-0330-0/24/05 \nhttps://doi.org/10.1145/3613904.3642517 CCS CONCEPTS \n• General and reference → Design; • Software and its engi-\nneering → Graphical user i", "source": "database"}, "2_6": {"id": "2_6", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nterfaces ; Object oriented frameworks ; • \nInformation systems → Multimedia and multimodal retrieval ; • \nHuman-centered computing → User interface programming . \nKEYWORDS \nmultimodal interactions, development frameworks, programming \nframework, large-language model, natural language processing \nACM Reference Format: \nJackie Junrui Yang, Yingtian Shi, Yuhan Zhang, Karina Li, Daniel Wan \nRosli, Anisha Jain, Shuning Zhang, Tianshi Li, James A. Landay, and Mon-\nica S. Lam. 2024. ReactGenie: A Deve", "source": "database"}, "2_7": {"id": "2_7", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "lopment Framework for Complex \nMultimodal Interactions Using Large Language Models. In Proceedings \nof the CHI Conference on Human Factors in Computing Systems (CHI ’24), \nMay 11–16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA, 23 pages. \nhttps://doi.org/10.1145/3613904.3642517 \n1 INTRODUCTION \nMultimodal interactions, combining multiple different input and \noutput modalities, such as touch, voice, and graphical user inter-\nfaces (GUIs), offer increased flexibility, efficiency, and adaptabil", "source": "database"}, "2_8": {"id": "2_8", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "-\nity for diverse users and tasks [52]. However, the development of \nmultimodal applications remains challenging for developers due \nto the complexity of managing multimodal commands and han-\ndling the low-level control logic for interactions. Existing frame-\nworks [12, 14, 32, 41, 42, 49, 50] often require developers to manually \nhandle these complexities, significantly increasing development \ncosts and time. The voice modality, in particular, presents a unique \nchallenge due to the composition", "source": "database"}, "2_9": {"id": "2_9", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ality and expressiveness of natural \n CHI ’24, May 11–16, 2024, Honolulu, HI, USA Yang et al. \nReactGenie Runtime\nDeﬁned StatesOrder.GetActiveCart().addItems(items:O\nrder.OrderHistory().matching(field:.restaurant,value:Restaurant.current())[0].items)\n!\nDeﬁne Object-\nOriented States\nTaco Bell\nTaco 3/3\nOrderRestaurant\nMr Sun 3/3\nOrder\nCrunchwrap\nFoodItem\nQuesadilla\nFoodItem\nTaro boba\nFoodItem\nDeﬁne \nUI Components \nRestaurant\nItemView\nOrderItem\nView\nFood\nThumbnail“Reorder my last meal \nfrom this re", "source": "database"}, "2_10": {"id": "2_10", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "staurant.”\nDeveloper-Coded GUI Generated Multimodal UISemantic \nParser\nInput\nUI MappingNLPL\nOutput\nUI Mapping ①\n ②\n④ \nTaco Bell\nRestaurantNew Taco\nOrderTaco 3/3\nOrder\nUI ReferenceResult③ \nFigure 1: ReactGenie allows developers to easily build multimodal applications by better-separating interfaces (UI components) \nfrom computational models (object-oriented state). The demo (two screenshots) shows the user performing a multimodal \n(speech + touch) command (left screenshot), with the system execut", "source": "database"}, "2_11": {"id": "2_11", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ing the command by parsing the voice, understanding \nthe reference in touch, and presenting the user with the appropriate UI interface and text feedback (right screenshot). (Left) \nReactGenie provides this new yet familiar interface to create a GUI by defining states (data and logic) and UI components \n(transformation from data to UI representation). (Right) ReactGenie automatically generates a natural semantic parser from \ndeveloper-defined states and generates input and output UI mappings from", "source": "database"}, "2_12": {"id": "2_12", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " developer-defined UI components. ReactGenie can \nthen execute rich multimodal commands by composing the methods and properties of states and presenting the results using \nexisting UI components. \nlanguage. Sub-par implementations often greatly reduce the expres-\nsiveness of these multimodal interfaces. Various systems [28, 53] \ncan automatically handle voice commands by converting them to \nUI actions, but they are prone to error and do not allow developers \nto fully control the app’s behavior. ", "source": "database"}, "2_13": {"id": "2_13", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "\nThe research described in this paper aims to provide developers \nwith a simple programming abstraction (see Figure 1) by hiding the \ncomplexity of natural language understanding and supporting the \ncomposition of different modalities automatically. Our goal is to \nenable users to access off-screen content/actions and complete tasks \nthat normally involve multiple GUI taps in a single multimodal \ncommand, as illustrated in Figure 2. This flexibility is achieved \nwith little additional effort fro", "source": "database"}, "2_14": {"id": "2_14", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "m developers compared to traditional \nGUI apps. This approach encourages the adoption of multimodal \ninteractions and makes multimodal interactions more accessible to \nend-users. \nThis paper presents ReactGenie1, a declarative programming \nframework for developing multimodal applications. The core con-\ncept behind ReactGenie is a better abstraction that separates the \nmultimodal input and output interfaces from the underlying compu-\ntation models. ReactGenie uses an object-oriented state abstrac", "source": "database"}, "2_15": {"id": "2_15", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "tion \nto represent the computation model of the app and uses declarative \nUI components to represent the UI. Users’ compound multimodal \ncommands are translated into a composition of multiple function \n1project website (including source code): https://jya.ng/reactgenie, https://hci.stanford. \nedu/research/reactgenie/ calls using large language models (LLMs), e.g., to find the referred \nobject/objects and make the right state change. \nExisting declarative UI state management frameworks, such as \n", "source": "database"}, "2_16": {"id": "2_16", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "Redux [6], use a single global state store to manage all of the state \nchanges of the UI. The straightforward way to implement rich mul-\ntimodal user commands in these existing frameworks is by making \nmany imperative-style function calls. However, these function calls \nrequire the error-prone creation of many intermediate variables to \nstore return values that are then used in the next function call as the \nprogrammer traverses the complex state stored in the monolithic \nobject. These intermedi", "source": "database"}, "2_17": {"id": "2_17", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ate variables commonly cause missing ref-\nerences to variables when the neural semantic parser translates the \nuser’s natural language input into code [36]. In contrast, the object-\noriented state abstraction in ReactGenie encourages componentized \nclasses instead of a single global state store. The componentized \nclasses result in smaller objects, each equipped with methods for \nrelevant operations. This design supports multiple chained method \ncalls/property accesses (method chaining) and prov", "source": "database"}, "2_18": {"id": "2_18", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ides a straight-\nforward representation of the user’s command with no need for \nintermediate variables (as shown in the example NLPL command \nin Figure 1). This allows ReactGenie to accurately compose the \nmethods and properties of existing states needed for executing rich \nmultimodal commands. \nWith ReactGenie, developers build graphical interfaces using \na development workflow similar to a typical React + Redux [5] \napplication. To add multimodality, the developer simply adds a few \nannotation", "source": "database"}, "2_19": {"id": "2_19", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "s to their code and example parses (pairs of expected  ReactGenie CHI ’24, May 11–16, 2024, Honolulu, HI, USA \nend-user voice command examples and the corresponding function \ncalls). These command examples indicate what methods/proper-\nties can be used in voice and how. By using the extracted class \ndefinitions and example parses from the developer’s state code, Re-\nactGenie creates a parser that leverages an LLM [17] to translate the \nuser’s natural language to a new domain-specific language (D", "source": "database"}, "2_20": {"id": "2_20", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "SL), \nNLPL, Natural Language Programming Language. Combined with \na custom-designed interpreter, ReactGenie can seamlessly handle \nmultimodal commands and present the results in the graphical UI \nthat the developer builds as usual. \nAs shown in Figure 1 left, developers can define both object-\noriented state abstraction classes to handle data changes and UI \ncomponents that explicitly map the state to the UI. Similar to React, \nwhen the user interacts with the app, the app’s state will be up-\nda", "source": "database"}, "2_21": {"id": "2_21", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ted, and the UI will be re-rendered. What sets ReactGenie apart \nis its unique ability to support rich multimodal input, as shown in \nFigure 1 right. \nThe main contributions of this research are as follows: \n• ReactGenie, a multimodal app development framework \nbased on an object-oriented state abstraction that is easy \nfor developers to learn and use and generates apps that sup-\nport rich multimodal interactions. \n• A programming language, NLPL, used to represent user’s \nmultimodal commands. Th", "source": "database"}, "2_22": {"id": "2_22", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "is involves the design of the high-\nlevel annotation of user-accessible functions, the automatic \ngeneration of a natural semantic parser using LLMs that \ntargets NLPL, a new DSL for rich multimodal commands, \nand an interpreter that executes NLPL. These systems sup-\nport automatic and accurate handling of natural language \nunderstanding in ReactGenie. \n• Evaluations of ReactGenie: \n– For developers, we demonstrated its expressiveness \nthrough building three representative demo apps in dif-\nfere", "source": "database"}, "2_23": {"id": "2_23", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nt domains, its low development cost by comparing \nit with GPT-3 function calling, and its usability and learn-\nability through a study with 12 developers successfully \nbuilding a demo app. \n– For end-users, we measured the parser accuracy to be 90% \nwith elicited commands from 50 participants and evaluated \nthe usability of apps built using ReactGenie in a user study \nwith 16 participants. We found users had a reduced cogni-\ntive load when using an app with ReactGenie-supported \nmultimodal inte", "source": "database"}, "2_24": {"id": "2_24", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ractions compared to using a graphical \nuser interface (GUI) app. They also preferred the multi-\nmodal app to the GUI-based app. \n1.1 Targeted Interactions \nReactGenie supports rich interactions that are complex for current \ncomputer systems, but are intuitive for users. One example of a rich \nmultimodal interaction is shown in the center of Figure 1: the user \nsays, “Reorder my last meal from this restaurant” while touching the \nrestaurant displayed on the screen. Such commands are common in \nh", "source": "database"}, "2_25": {"id": "2_25", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "uman-to-human communication. Still, they involve multiple steps \n(retrieving the history of orders from the restaurant, creating an \norder, and adding food to the order) for the app. These commands are complex to implement today as they require combining inputs \nfrom both modalities and/or composition of different features. \nReactGenie supports a typical family of gesture + speech multi-\nmodal interactions. This aligns with one of the categories of speech \nand gesture multimodal applications pro", "source": "database"}, "2_26": {"id": "2_26", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "posed by Sharon Oviatt’s \nseminal work [9]: The recognition modes ReactGenie supports are \nsimultaneous and individual modes, meaning that ReactGenie sup-\nports users to use either speech-only interactions, gesture-only \ninteractions, or both interactions at the same time (“What is the last \ntime I ordered from this [touch on a restaurant] restaurant”). The \nsupported gesture input type is touch/pen input, and the size of the \ngesture vocabulary is a deictic selection . This means that ReactGeni", "source": "database"}, "2_27": {"id": "2_27", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e \nfocuses on scenarios where the user’s gesture input resolves object \nreferences through pointing in a multimodal command. The size \nof speech vocabulary is arbitrary human sentences, and the type of \nlinguistic processing is large-language model processing. The last \ntwo terms are new types we invented to better describe ReactGe-\nnie’s support for rich commands and the use of highly generalizable \nlarge-language models. Following Oviatt’s original classification, \nReactGenie would be classifi", "source": "database"}, "2_28": {"id": "2_28", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ed as large vocabulary and statistical \nlanguage processing. ReactGenie uses late semantic fusion to fuse \ninput from different modalities, which means the system integrates \nand interprets the meaning of inputs from multiple modalities only \nafter each input has been independently processed and understood. \nWith ReactGenie, the developer simply provides a small amount \nof additional information associated with each input method and \nfunction. Our system supports the full compositionality of inp", "source": "database"}, "2_29": {"id": "2_29", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ut \nmodalities and functions by automatically translating a user com-\nmand into one of exponentially many possible action sequences. \nThe richness of user interaction afforded by our system is unprece-\ndented, as traditional multimodal programming frameworks require \ndevelopers to hard-code every combination of features supported. \nReactGenie lets the programmer simply describe the functional-\nity of their code, including actions they support and the relationship \nbetween UI and data. This allow", "source": "database"}, "2_30": {"id": "2_30", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "s ReactGenie to handle these rich \nmultimodal commands in arbitrary combinations of actions with-\nout requiring direct developer input. The example in Figure 1 is \nsupported by: \n(1) ReactGenie first translates the user’s voice command to the \nNLPL code. For example, the user refers to an element in \nthe UI by voice (“this restaurant”), and the semantic parser \ngenerates a special reference Restaurant.current(). \n(2) ReactGenie extracts the tap point from the UI and uses the UI \ncomponent code t", "source": "database"}, "2_31": {"id": "2_31", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "o map the tap point back to a state object \nRestaurant(name:\"TacoBell\"). \n(3) With the parsed DSL and UI context, ReactGenie’s interpreter \ncan execute the generated NLPL using developer-defined \nstates. It first retrieves the most recent order from “Taco \nBell”, designated as “Taco 3/3”. Then, it creates a new order, \ndesignated as “New Taco”. Finally, the interpreter adds all \nthe food items from “Taco 3/3” to “New Taco” and returns \nthe new order. \n(4) ReactGenie passes the return value of th", "source": "database"}, "2_32": {"id": "2_32", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e NLPL statement \nto the output UI mapping module. Because the return value \nis an Order object, ReactGenie searches in the developer’s \nUI component code to find a corresponding representation  CHI ’24, May 11–16, 2024, Honolulu, HI, USA Yang et al. \n(Output UI Mapping) to present the result to the user. React-\nGenie also generates a text response using the LLM based on \nthe user’s input, parsed NLPL, and the return value: “Your \ncart is updated with the same order from this restaurant as \nthe ", "source": "database"}, "2_33": {"id": "2_33", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "last time.” \nDuring this process, the ReactGenie framework uses its knowl-\nedge about the developer’s app to automatically understand a mul-\ntimodal compositional command, compose actions to execute, and \nfind the appropriate interface to present the results to the user. This \npipeline allows ReactGenie to handle more commands than prior \nframeworks with little developer input. \n2 RELATED WORK \nIn this section, we review related work on multimodal interaction \nsystems, Graphical and Voice UI fra", "source": "database"}, "2_34": {"id": "2_34", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "meworks, and multimodal in-\nteraction frameworks. \n2.1 Multimodal Interaction Systems \nMany researchers have proposed multimodal interaction systems. \nThe earliest multimodal interaction systems, such as Bolt’s “Put-\nthat-there”, were developed in the 1980s [15]. They demonstrated \nthat users can interact with a computer using voice and gestures. \nQuickSet [21] further demonstrated use cases of multimodal in-\nteraction on a mobile device and showed military and medical \napplications. \nRecent wor", "source": "database"}, "2_35": {"id": "2_35", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "k has explored different applications of multimodal \ninteraction, including care-taking of older adults [48, 51], photo \nediting [38], and digital virtual assistants [33]. Researchers have \nalso explored different devices and environments for multimodal \ninteraction, including augmented reality [59], virtual reality [39, 56], \nwearables [16], and the Internet of Things [25, 34, 55, 58]. \nThese projects have demonstrated the great potential of mul-\ntimodal interaction systems. However, multimodal", "source": "database"}, "2_36": {"id": "2_36", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " systems still \nhave limited adoption in the real world due to the development \ncomplexity they currently require. \n2.2 Graphical UI frameworks \nReactGenie is built on top of an existing graphical UI framework to \nprovide a familiar development experience. Model–view–controller \n(MVC) [35] is the traditional basis of UI development frameworks \nand is used in frameworks such as Microsoft’s Windows Forms [29], \nand Apple’s UIKit [1]. The model stores data while the controller \nmanages GUI input an", "source": "database"}, "2_37": {"id": "2_37", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "d updates the GUI view based on data changes. \nTypically implemented in object-oriented programming languages, \nMVC can be compared to a shadow play, where objects (controllers) \nmanipulate GUIs and data to maintain synchronization. However, \nupdating the model with alternative modalities, such as voice, is \nnot feasible due to the strong entanglement between models and \nGUI updates. \nGarnet [43, 45], a user interface development environment in-\ntroduced in the late 1980s, is another notable app", "source": "database"}, "2_38": {"id": "2_38", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "roach to GUI \ndevelopment. Garnet introduced concepts like data binding, which \nallows the GUI to be updated automatically when the data changes. \nIt also tries abstracting the GUI state away from the presentation \nusing interactors [44]. While interactors allow the UI state to be \nrewired and thus to be updated using another modality like voice or gesture [37], they do not enable manipulation of more abstract \nstates (e.g., foods in a delivery order) that are not directly mapped \nto a single UI", "source": "database"}, "2_39": {"id": "2_39", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " control. \nDeclarative UI frameworks, such as React [2], Flutter [3], and \nSwiftUI [7], are a more recent approach to UI development. With \ndeclarative UI frameworks, programmers write functions to trans-\nform data into UI interfaces, and the system automatically manages \nupdates. To ease the management of states that may be updated \nby and reflected on multiple UI interfaces, centralized state man-\nagement frameworks, such as Redux [6], Flux [10], and Pinia [4], \nare often used together with th", "source": "database"}, "2_40": {"id": "2_40", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ese declarative UI frameworks. They \nprovide a single source of truth for the application state and allow \nstate updates to be reflected across all presented UIs. This approach \ncan be likened to an overhead projector, where the centralized state \nrepresents the writing and the transform functions represent the \nlens projecting the UI to the user. While this approach improves \nseparation and UI updating, it sacrifices the object-oriented nature \nof the data model. This centralized state works we", "source": "database"}, "2_41": {"id": "2_41", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ll with button \npushes but comes short in dynamically composing multiple actions \nto support rich multimodal commands. \nReactGenie reintroduces object-orientedness to centralized state \nmanagement systems by representing the state as a sum of all \nclass instances in the memory. Developers can declare classes and \ndescribe actions as member functions of the classes. ReactGenie cap-\ntures all instantiated classes and stores them in a central state. This \nmore modularized model is analogous to acto", "source": "database"}, "2_42": {"id": "2_42", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "rs (class instances) in a \nmovie set, with views (UI components) acting as cameras capturing \ndifferent angles of the centralized state. In this way, ReactGenie \nenables rich action composition through type-checked function \ncalls. Furthermore, developers can tag specific cameras to point at \ncertain objects, enabling automatic UI updates from state changes. \nThese features allow ReactGenie apps to easily support the com-\npositionality of multimodal input and enable the interleaving of \nmultimod", "source": "database"}, "2_43": {"id": "2_43", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "al input with other graphical UI actions. \n2.3 Voice UI frameworks \nCommercial voice or chatbot frameworks, such as Amazon Lex, \nGoogle Dialogflow, and Rasa, are designed to handle natural lan-\nguage understanding and generation. These frameworks allow de-\nvelopers to define intents and entities and then train the model to \nrecognize the intents and entities from the user’s input. In this con-\ntext, intents refer to categories of the user’s action, such as making \na reservation or asking for wea", "source": "database"}, "2_44": {"id": "2_44", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ther information, and one action \ncan only be mapped to one intent. Intents are usually mapped to \ndifferent programming implementations to handle commands in \nthe corresponding intent categories. These frameworks require a \ncomplete redevelopment of an application to support voice-only \ninput. Frameworks such as Alexa Skills Kit and Google Actions \nallow developers to extend existing applications to support voice \ninput. However, these still require manual work to build functions \nonly for voic", "source": "database"}, "2_45": {"id": "2_45", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e, and the visual UI updates are limited to simple text \nand a few pre-defined UI elements. Additionally, the one-intent-\none-implementation nature of the intent-based architecture limits \nthe compositionality of the voice commands. \nResearch-focused voice/natural language frameworks, such as \nGenie [19, 54] and other semantic parsers [13, 46], are designed to  ReactGenie CHI ’24, May 11–16, 2024, Honolulu, HI, USA \nOﬀ-screen content Oﬀ-screen action Multiple actions/content\n“Show me the lock in", "source": "database"}, "2_46": {"id": "2_46", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " \nthe same room.”“Share this video.”\n“Share this account.”\n“Share this comment.”\n…\n!\n!\n!\n“Reorder what I \nordered last time.”\nFigure 2: ReactGenie’s targeted interaction scenarios. \nsupport better compositionality of voice commands. However, given \nthat today’s app development is primarily geared toward mobile \nand graphical interfaces, these frameworks require extra work from \nthe developer and do not support multimodal features. ReactGenie \nimproves this experience by integrating the developme", "source": "database"}, "2_47": {"id": "2_47", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nt of voice \nand graphical UIs, allowing developers to extensively reuse existing \ncode and support multimodal interactions. \n2.4 Multimodal Interaction Frameworks \nPrior work has also proposed multimodal interaction frameworks \nthat allow developers to build multimodal applications. One of the \nearliest works is presented by Cohen et al. [20]. It includes ideas \nlike forming the user’s voice command as a function call and using \nthe user’s touch point as a parameter to the function call. Later,", "source": "database"}, "2_48": {"id": "2_48", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " \nresearchers created standards [23, 24] and frameworks [12, 14, 32, \n41, 42, 49, 50] to help developers build apps that can handle multiple \ninputs across different devices. Although these frameworks provide \nscaffolding for developers to build multimodal applications, they \nmostly treated voice as an event source that can trigger functions \nthe developer has to explicitly implement for voice. Developers also \nhave to manually update the UI to reflect the result of the voice \ncommand. This manu", "source": "database"}, "2_49": {"id": "2_49", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "al process limits voice commands to simple \nsingle-action commands and makes it difficult for developers to \nbuild richer multimodal applications. \nRecently, there are research projects on generating voice com-\nmands by learning from demonstration [27, 40, 47], extracting from \ngraphical user interfaces with large language models [28, 53], or \nbuilding multimodal applications using existing voice skills [57]. \nThe first approach still requires developers to manually create \ndemonstrations for ea", "source": "database"}, "2_50": {"id": "2_50", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ch action and limits the compositionality \nof the voice commands. The second approach is useful for accessi-\nbility purposes, but it relies on the features being easily extractable \nfrom the GUI. It is uncertain how well the first two approaches \ncan generalize to more complex UI tasks that require multiple UI \nactions. The third approach is constrained by what is provided by the voice skills and, traditionally, these have been limited due to \nthe added development effort. \nIn comparison, ReactG", "source": "database"}, "2_51": {"id": "2_51", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "enie leverages the existing GUI devel-\nopment workflow and requires only minimal annotations to the \ncode to generate multimodal applications. Having access to the \nfull object-oriented state programming codebase, ReactGenie can \nhandle the natural complexity of multimodal input, compose the \nright series of function calls, and update the UI to reflect the result \nautomatically. \n3 SYSTEM DESIGN \nIn this section, we first define the design goals of the framework. \nThen, we describe the theory of", "source": "database"}, "2_52": {"id": "2_52", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " operation that addresses the de-\nsign goals. Finally, we discuss the implementation of the system \ncomponents and workflow. \n3.1 Design goals \nOur design goals include aspects of the interaction design of React-\nGenie apps as well as the design of the framework itself. \n3.1.1 Interaction Design. ReactGenie is primarily designed to en-\nhance user interaction with mobile applications, but the concept \nshould also apply to apps on other platforms. Today, mobile applica-\ntions are well-optimized fo", "source": "database"}, "2_53": {"id": "2_53", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "r touch and graphical interactions. Users \ncan use the graphical interface to see content on the screen and \nuse touch to access actions on the screen. To further enhance the \nuser’s performance and reduce cognitive load, ReactGenie focuses \non supporting interactions that often involve touch actions used \ntogether with a voice command. \nHere is a series of example commands in interactions with a food \nordering app between user A and their friend user B: \nC1 A knows what they want, so A says, “S", "source": "database"}, "2_54": {"id": "2_54", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "how me what I ordered \nlast week from McDonald’s.” The app responds with the order \nhistory.  CHI ’24, May 11–16, 2024, Honolulu, HI, USA Yang et al. \nC2 A wants to add a previously ordered food into the cart (not \navailable on UI). A says, “Order this hamburger,” with a tap \non the “Big Mac” entry in the order history, and the app adds \na “Big Mac” to the shopping cart. \nC3 B wants to order something different, so they tap on the \nrestaurant to view the menu. \nC4 B doesn’t like beef, so they sa", "source": "database"}, "2_55": {"id": "2_55", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "y, “Show me food without beef,” \nthe app displays options accordingly. \nC5 B says, “Order a meal with this sandwich,” with a tap on the \n“McChicken sandwich,” the app adds a McChicken meal to \nthe cart. \nThis interaction demonstrates the power of these multimodal \ncommands where voice and touch are used interchangeably or \nin conjunction. These commands can be categorized into three \ninteraction design goals (see Figure 2): \nI1 Access off-screen content (C1, C4): For example, the user is \nlookin", "source": "database"}, "2_56": {"id": "2_56", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "g at the smart home app and notices abnormal motion \non the smart home app’s living room security camera. So they \ncan talk to the smart home app, “Show me the lock status \nhistory in the same room of this device.” This interaction \nusually requires multiple GUI navigation steps (go to the \ndevice’s room page, navigate to the door lock section, check \ndoor lock history) to access the content. \nI2 Access off-screen actions (C2): For example, the user says, \n“Share this creator/comment” while watc", "source": "database"}, "2_57": {"id": "2_57", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "hing a YouTube \nvideo. Some actions are hidden behind a menu or a button, \nand some are not accessible at all on mobile devices. \nI3 Combine multiple actions/content (C5): For example, the \nuser says, “Order what I ordered last time” while looking at \na food delivery app. This usually requires the user to go back \nand forth between an order detail page and a menu page. \nThe common theme among these interactions is that they require \nthe multimodal interaction framework to understand the content ", "source": "database"}, "2_58": {"id": "2_58", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "\nand actions available in the app. For content, the framework needs \nto know what is the content on the screen, how to access it, and \nhow to represent returned content (from user-initiated commands) \nto show the retrieved content. For actions, the framework needs to \nknow the list of available actions and how to render changes on the \nuser interface after the action is triggered. Finally, the framework \nneeds to translate the user’s intent, which may be rich, into possibly \na series of actions ", "source": "database"}, "2_59": {"id": "2_59", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "and content displays. \nWith ReactGenie, apps will have a microphone button on the \nscreen. When the user taps on the button, the user can say their \ncommand and refer to the content on the screen by tapping it. The \napp will then parse the voice command and touch input and exe-\ncute the corresponding actions to help the user with the scenarios \ndescribed above. \n3.1.2 Framework Design. To translate the content and actions in \nthe user’s rich multimodal intent, ReactGenie needs to obtain infor-\nm", "source": "database"}, "2_60": {"id": "2_60", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ation about the app’s capabilities from the code. The design goal \nfor ReactGenie is to do this in a way that causes minimal disruption \nfor the application developer. \nWithout a proper framework for multimodal apps, the developer \nmust design their own mechanisms to handle voice, handle the \ncomplexity of multimodal commands, and maintain control of the app’s behavior. Our goals for the ReactGenie framework include \nhandling these issues: \nF1 Maintain the expressiveness of the developer for the", "source": "database"}, "2_61": {"id": "2_61", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ir \nGUI appearances and specific app functionality. \nF2 Reduce development cost by maximizing the reuse of \nexisting GUI code and hiding the complexity of handling \nmultimodal commands from developers. \nF3 Ease the learning curve by providing a similar program-\nming experience to existing GUI frameworks. \n3.2 Theory of Operation \nReactGenie presents an object-oriented state programming model \nto the developer. State code, in the context of GUI development, \nrefers to the part of the application ", "source": "database"}, "2_62": {"id": "2_62", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "that manages the data and logic \nthat determine the state of the user interface. The global state store \nin Redux is typically represented as a tree of stored variables and \nthe associated functions to transform them. The concept of state in \ndeclarative UIs is similar to the model in the model-view-controller \n(MVC) programming models. \nAs mentioned in Section 2.2, in frameworks like React, UI devel-\nopment is moving towards separating the UI from the state. Devel-\nopers define functions (compo", "source": "database"}, "2_63": {"id": "2_63", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nents ) that convert the state into UI. \nEach UI component receives part of the state and renders the UI in \nan HTML-like format. Components can sometimes render a part of \ntheir state using another component in a compositional way (e.g., \na restaurant menu component can use menu item components to \nrender each food item, and the restaurant menu components host a \nlist to organize the menu item components). Therefore, there is a \nunidirectional data flow from the state to the UI so that the stat", "source": "database"}, "2_64": {"id": "2_64", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e \nupdate is automatically reflected on the UI without any additional \ncode from the developers. In comparison, in a typical MVC para-\ndigm, the controller simultaneously updates the model (state) and \nthe UI to keep them in sync. The unidirectional data flow feature \nin declarative UIs (compared to MVC) allows data to be updated \noutside of GUI input because data updates are no longer entangled \nwith UI update code in MVC’s controller code. This feature allows \nReactGenie to use multimodal comm", "source": "database"}, "2_65": {"id": "2_65", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ands to change the same state \nand update the UI accordingly, maximizing the reuse of existing \nGUI code (F2). \nHowever, in these existing declarative UI frameworks, the state \nis represented by a single global data store. This data store contains \na tree-like data structure with all the content and status of the \napp (state) and a list of functions to manipulate the state data. The \nfunctions usually contain parameters for indexing into the state \nobject and parameters to further specify the ac", "source": "database"}, "2_66": {"id": "2_66", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "tions. For example, an \nADD_FOOD_TO_ORDER action needs to have the parameters of food_-\nid, order_id , and quantity . This works well for GUI design, as \ndevelopers can implement an action for each button press, and each \nbutton in the GUI stores the corresponding ID that it needs to call \nwhen pressed. However, this makes it difficult to handle typical \nmultimodal commands, which require composing multiple actions \n(I3). To translate a user command of “Add two hamburgers to a \nnew order”, an ex", "source": "database"}, "2_67": {"id": "2_67", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ample translated program in React-Redux would \nbe: \n// create the order \ndispatch(CREATE_ORDER())  ReactGenie CHI ’24, May 11–16, 2024, Honolulu, HI, USA \nReact-Redux’s Monolithic State Implementation ReactGenie’s Object-Oriented State Implementation\nexport const orderReducer = (state = {orders: []}, action: any) => {\n    switch (action.type) {        case FETCH_ORDERS:            return {...state, orders: fetchOrdersFromServer()};        case CREATE_ORDER:            const newOrder =           ", "source": "database"}, "2_68": {"id": "2_68", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "      {id: state.orders.length + 1, items: [], date: Date.now()}            return {...state,orders: [...state.orders,newOrder]};        case ADD_FOOD_TO_ORDER:            const {foodId} = action.payload;            const updatedOrders = state.orders.map((order) => {                if (order.id === state.orders.length) {                    return {                        ...order,items: [...order.items, {id: foodId}]                    };                }                updateServer();          ", "source": "database"}, "2_69": {"id": "2_69", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "      return order;            });            return {...state,orders: updatedOrders,};        default:            return state;    }};\n@GenieClass(\"Past order or a shopping cart\")class Order extends DataClass {    @GenieKey()    public orderId: string;    @GenieProperty(\"Items in the order\")    public orderItems: FoodItem[];    @GenieProperty(\"When order is created\")    public orderDate: DateTime;    constructor({orderId, orderItems, orderDate}: {orderId: string, orderItems: FoodItem[], orderDa", "source": "database"}, "2_70": {"id": "2_70", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "te: DateTime}) {        super({orderId, orderItems}); this.orderId = orderId; this.orderItems = orderItems; this.dateTime = DateTime();     }    @GenieFunction()    All(): Order[] {        return fetchOrdersFromServer();    }    @GenieFunction(\"Create a new order\")    static CreateOrder(): Order {        return new Order({orderId: randomId(), orderItems: []});    }    @GenieFunction(\"Add an item to the order\")    addItem({foodItem}: {foodItem: FoodItem}) {        this.orderItems.push(foodItem); ", "source": "database"}, "2_71": {"id": "2_71", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "updateServer();    }}// Example parses omitted here, see appendix\nFigure 3: A comparison between state code in React-Redux and in ReactGenie. (Left) Part of an example state code in Redux. \nData is stored in the state variable, and the state can be mutated by the actions defined. These actions (stored in a Reducer) do \nnot have explicit types, and they directly manipulate the state, so no return values are defined. Note that the return values of \ncase statements in a Reducer indicate the new sta", "source": "database"}, "2_72": {"id": "2_72", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "te variable after the state changes; actions do not have return values. Due \nto its monolithic design, it is hard to compose functions together to achieve some multimodal actions. (Right) Part of an \nexample state code in ReactGenie. Automatically managed by ReactGenie, the state is composed of all the instantiated objects’ \nDataClasses. Actions in the state code are defined as methods of the class. All the methods have explicit parameter types and \nreturn types. These functions can be composed ", "source": "database"}, "2_73": {"id": "2_73", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "together to achieve multimodal actions. \n// find the id of the created order \nconst order_id = store.orders.last().id \n// find the food id of hamburger \nconst food_id = store.foods. \nfilter(food => food.name === 'hamburger')[0].id \n// add food to order \ndispatch(ADD_FOOD_TO_ORDER(order_id, food_id, 2)) \nThis process involves the creation of multiple intermediary variables \nand queries to the state object. When the neural semantic parser \ngenerates code for this process, we have found that LLMs w", "source": "database"}, "2_74": {"id": "2_74", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ill often \ngenerate a line of code referencing an intermediary variable that \nhas not been declared before, causing an error in response to the \nuser’s query. The problem is that even if we have already created a \nnew order, we would have to retrieve the order ID from the state \nobject, save it in an intermediary variable, and feed that ID into \nthe imperative style actions. This issue is solved in ReactGenie by \napplying concepts of object-oriented programming where an order \ncan be represented", "source": "database"}, "2_75": {"id": "2_75", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " by a separate object that has both the data and \nall of the relevant actions (e.g., adding a food item to the order) \nassociated with the object, so a retrieved order can directly be used \nto call the add food item action. \nSo, ReactGenie introduces the object-oriented programming \nmodel to componentize the state of a declarative UI app. In \ncomponentizing the state object, developers implement smaller \nobjects containing its relevant content stored in properties and \nactions defined as methods", "source": "database"}, "2_76": {"id": "2_76", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": ". This componentized design allows \nReactGenie to translate a typical multimodal command into a \nsingle statement with method chaining. Using the example above, the user’s command can be translated as: Order.CreateOrder \n().addFoodItem(foodItem:FoodItem.All().matching(field \n:.name,value:\"hamburger\"),quantity:2 . With ReactGenie’s \nstate abstraction, LLMs can generate code that directly calls \naddFoodItem after creating the new order. These methods are also \nstrictly typed, which helps the natur", "source": "database"}, "2_77": {"id": "2_77", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "al semantic parser develop \nthe correct combination of methods with fewer runtime errors. \nThis succinctness improves the accuracy of the neural semantic \nparser that translates the user’s natural language command to \nexecutable code. Furthermore, as the GUI and voice modality \nshare the same content and state, this representation supports \ninterchangeability in user input modality for each part of the rich \nmultimodal command. \nIn practice, to work with ReactGenie’s object-oriented state \nabstr", "source": "database"}, "2_78": {"id": "2_78", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "action, the developer identifies the user-accessible content \n(object or object properties) and actions (functions) with the \nGenieProperty and GenieFunction annotations, respectively, \nalong with an example of how it may be referred to in English \nas shown in Figure 3 right. The GUI is programmed using compo-\nsitional components (similar to other declarative UI frameworks \ndescribed above) rendered from the user-accessible state objects. \nThis allows the internal state to be rendered to the use", "source": "database"}, "2_79": {"id": "2_79", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "r in a GUI. \nThe high-level model of ReactGenie resembles popular GUI de-\nvelopment frameworks (React + Redux), which makes it easy for \ndevelopers to learn and use (F3). ReactGenie automatically han-\ndles the retrieval of content (objects/properties) off-screen (I1), the \nexecution of actions off-screen (I2), and combinations of the two \n(I3). In this way, voice and multimodal commands will be handled  CHI ’24, May 11–16, 2024, Honolulu, HI, USA Yang et al. \nSystem outputState\nComponentsRuntime", "source": "database"}, "2_80": {"id": "2_80", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "Transpilation & Initialization TimeState \nAnnotations\nComponent \nWrapperSemantic Parser\nResponse \nGenerator\nInput/Output\nUI MappingGenerated PromptClass deﬁnition\nExample parses\nUser inputVoice\nTouchSemantic Parser\nNLPL InterpreterInput UI Mapping\nState\nContent in UI Output UI MappingText\nTouch\nPointsReferred\nInstanceNLPL\nFunction &\nProperty\nResponse \nGeneratorFeedback in \nText\nComponentReturn\nValueChained\nExecutionSteps\nDeveloper’s code@GenieClass(\"Email address\")\nclass Email extends HelperClas", "source": "database"}, "2_81": {"id": "2_81", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "s {    @GenieProperty(\"Email address\")    public email: string;    constructor({email}: {email: string}) {        super({email}); this.email = email;    }}\n@GenieClass(\"Signature Request\")class SharedDoc extends DataClass {    @GenieKey()    public signatureRequestId: string;    @GenieProperty(\"Recipient of the signature request\")    public recipient: Email;    @GenieProperty(\"Document to be signed\")    public document: Document;\n    constructor({signatureRequestId: string, recipient: Email, \ndo", "source": "database"}, "2_82": {"id": "2_82", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "cument: Document}) {    }\n    @GenieFunction(\"Create a new shared document\")\n    static CreateSharedDoc({recipient: Email, document: Document}): SharedDoc {        return new SharedDoc();    }}// Example parses omitted here, see appendix\nconst EmailView = GenieComponent(\"EmailView”, (email: Email) => { return  <View>{email.email  </View>});const ShareDocView = GenieComponent(“ShareDoc”, (sharedDoc: ShareDoc) => { return   <View>  <DocumentView doc={sharedDoc.document}/>  <EmailView email={shared", "source": "database"}, "2_83": {"id": "2_83", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "Doc.email}/>  </View>});\nconst MainView = () => {    const docs = multiSelector(()=> ShareDoc.All())    return <View>        {docs.map((doc)=><ShareDocView sharedDoc={doc}/>)}    </View>}HelperClass\nGenieComponentDataClass\nGenieComponent\nReactComponent\nFigure 4: Overview of the ReactGenie system: (Left) Developers write object-oriented state code for programming content and \nactions and define the UI as cascading components. (Right top) ReactGenie operates at transpilation and initialization tim", "source": "database"}, "2_84": {"id": "2_84", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e to \ngenerate runtime modules. (Right bottom) Developer modules, generated modules, and ReactGenie modules come together to \nprocess rich multimodal commands from the user. This workflow is similar to regular GUI development, maximizes code reuse, \nand allows full control of the app behavior. \nby LLM-generated programs and touch commands can be handled \nin the traditional way where developers write programs to handle \neach individual user input event. \nBy reusing a programming model similar to ", "source": "database"}, "2_85": {"id": "2_85", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "existing UI frame-\nworks, ReactGenie allows developers to control the look and feel of \ntheir app to a similar degree as typical declarative UI frameworks. \nMeanwhile, developers can also control what function the end-\nusers can access in multimodal interactions through whether or not \nto add annotations. Hence, developers have full control over the \nfunctionality of the app. These designs help developers maintain \nfull control of their app while enjoying the benefits multimodal \ninteraction off", "source": "database"}, "2_86": {"id": "2_86", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ers to end-users (F1). \n3.3 The Developer’s Programming Model \nFrom the developers’ point of view, using ReactGenie is similar to \nany declarative UI framework. They need to implement the state \ncode that provides the content and actions supported in the app. \nThey also need to specify the UI components that translates current \nstate classes into UI interfaces that the user can see and manipulate. \nTable 1 provides a list of functions and annotations that developers \nneed to provide for ReactGen", "source": "database"}, "2_87": {"id": "2_87", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ie (see Figure 4 left for an example). In Section 3.5, we will describe in detail the right side of Figure 4, \nwhich is how the ReactGenie system uses the developer-supplied \ncode described in this section, (1) transpiles (source-to-source com-\npiles) it into ReactGenie modules (Right top), and (2) uses the gener-\nated ReactGenie modules to handle end-users’ input (Right bottom). \n3.3.1 State Code. Developers provide the content and actions in \nan app through ReactGenie’s object-oriented state m", "source": "database"}, "2_88": {"id": "2_88", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "odel, imple-\nmented in TypeScript2. As with all object-oriented programming \nmodels, ReactGenie’s state code includes the definition and im-\nplementation of classes. Classes have declarations, methods, and \nproperties, which can be labeled as GenieClass , GenieFunction , \nand GenieProperty . These annotations in the state code (or State \nAnnotations for short) indicate that they are user-accessible via mul-\ntimodal commands. All ReactGenie annotations have an optional \nparameter that denotes the", "source": "database"}, "2_89": {"id": "2_89", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " purpose of that class/function/property, \nsimilar to comments in code. These code annotations or decorators \nin TypeScript are tags written before the class, method, and prop-\nerty declarations. This allows the relevant annotation code to be \nexecuted at initialization time to transform the capabilities of the \nannotated classes or methods. \n2https://www.typescriptlang.org/  ReactGenie CHI ’24, May 11–16, 2024, Honolulu, HI, USA \nTable 1: Annotations and Functions for programming ReactGenie. \nA", "source": "database"}, "2_90": {"id": "2_90", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nnotation/Function \nGenieClass \nGenieFunction \nGenieProperty GenieKey constructor Location \nDataClass/ \nHelperClass \nDataClass/ \nHelperClass \nDataClass/ \nHelperClass \nDataClass \nDataClass/ \nHelperClass Type \nAnnotation \nAnnotation \nAnnotation \nAnnotation \nFunction Parameters \ncomment (optional): describing \nthe class’s purpose. \ncomment (optional): describing \nthe function’s purpose. \ncomment (optional): describing \nthe property’s purpose. None required. \nVaries based on class require-\nments. Pu", "source": "database"}, "2_91": {"id": "2_91", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "rpose \nIndicates the class to be user-accessible through \nmultimodal interactions, essential for determin-\ning which parts of the code are relevant to the \nmultimodal experience. \nTags a method to be exposed to multimodal \ninteraction, marking this method as something \nthe user may call directly. \nExposes the property for multimodal interac-\ntion, letting users interact with this property \nthrough multimodal commands. \nIdentifies a unique ID property that uniquely \nidentifies an instance within ", "source": "database"}, "2_92": {"id": "2_92", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "a DataClass, crucial \nfor managing and retrieving data instances. \nInitializes a DataClass instance with required \ndata, setting up the basic data structure of an \nobject. \nGenieComponent UI Component Wrapper \nFunction target: DataClass or Helper-\nClass to be displayed. \ncomponent : The UI compo-\nnent to be wrapped. Wraps a UI component and maps it to a Data-\nClass or HelperClass instance, telling how to \npresent data/state visually and enable touch ref-\nerences in multimodal interactions. \nAll ", "source": "database"}, "2_93": {"id": "2_93", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "states that are accessible via multimodal interactions must \nbe declared as instances of the DataClass and HelperClass pro-\nvided by ReactGenie. A DataClass stores the app’s data, and a \nHelperClass provides definitions to ease the user’s interaction \nwith the data. The properties of a DataClass can be of TypeScript \nprimitive types, a DataClass, or a HelperClass. \nFigure 4 left shows an example of a document signing app; ex-\namples of each class are shown in the top left. As shown in the \nfigur", "source": "database"}, "2_94": {"id": "2_94", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e, the SharedDoc DataClass tracks the lifecycle of a docu-\nment’s signature request. An Email class is a HelperClass that \nhelps manage users’ email addresses. ReactGenie also has system-\nprovided HelperClass such as DateTime and TimeDelta to help \ndevelopers manage date, time, and period of time. The introduc-\ntion of the HelperClass class not only allows developers to define \nhelper functions but also helps with type-checking, which is use-\nful for developers to write less buggy code and for R", "source": "database"}, "2_95": {"id": "2_95", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "eactGenie’s \nsemantic parser to generate more correct NLPL. \nAll DataClass es start with the class declaration, which begins \nwith the GenieClass annotation and then the class name and a re-\nquired inheritance of DataClass . See the first two lines of ShareDoc \nin Figure 4. Developers need to implement one method and one \nproperty: \n(1) constructor method: The constructor of the class initializes \nthe instance with all the required data. \n(2) id property: A unique identifier of the instance, ann", "source": "database"}, "2_96": {"id": "2_96", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "otated \nby the GenieKey. \nDevelopers can also implement additional functions and prop-\nerties to complete the DataClass . If developers want the user \nto be able to use the functionality directly, they need to add the GenieFunction and GenieProperty annotation. An exam-\nple of that can be found in Figure 4 left (see document and \nthe CreateSharedDoc in SharedDoc ). Sometimes, developers may \nwant to implement internal housekeeping functions, such as clear-\ning the application cache or managing i", "source": "database"}, "2_97": {"id": "2_97", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nternal database transac-\ntions. Those functions/properties should not be annotated and will \nnot be called by ReactGenie. \nThe HelperClass allows developers to define new types \nthat can be used in the DataClass . A specific example is the \nReactGenie-supplied DateTime HelperClass . It can support op-\nerations like offsetting the time by a certain amount or set-\nting the year/month/day/hour/minute/second/day of the week \nto a certain value. It allows commands such as “last Thursday” \nto be tran", "source": "database"}, "2_98": {"id": "2_98", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "slated to “DateTime.Current.offset(week:-1).set \n(weekOfTheDay:4) ”. The developer can define other HelperClass \ninstances to support more complex operations such as length \nunit conversion. Similarly, the HelperClass needs to have a \nGenieClass annotation and needs to inherit from HelperClass . \nThe HelperClass only requires a constructor method that takes \nthe data as input and initializes the instance. ReactGenie will also \ngenerate a Current property for the HelperClass that returns the \nins", "source": "database"}, "2_99": {"id": "2_99", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "tance that is being referred to by the user. ReactGenie does not \nkeep track of the instances of HelperClass separately in memory, \nbut instead, they will be part of the DataClass that uses them. \nFor both DataClass and HelperClass , the developer can de-\nfine a description method to customize the string representation \nof the instance for response generation. By default, ReactGenie \nwill generate a JSON-like representation using all the instance’s \nproperties.  CHI ’24, May 11–16, 2024, Honolul", "source": "database"}, "2_100": {"id": "2_100", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "u, HI, USA Yang et al. \nFinally, developers need to provide example parses for the \nDataClass and HelperClass . Example parses or few-shot exam-\nples are pairs of expected end-user voice command examples and \nthe corresponding translated NLPL. Developers provide them as \nfew-shot examples for ReactGenie’s neural semantic parser. These \nexamples are helpful for the parser to learn about what the app \nis about and the syntax of NLPL. In practice, around 10 example \nparses are sufficient. Developer", "source": "database"}, "2_101": {"id": "2_101", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "s do not have to cover all use cases, \nand ReactGenie’s semantic parser can automatically generalize to \nmost of the app’s features and user expressions. \n3.3.2 Built-in Dataclass Methods. ReactGenie automatically gener-\nate three methods for each DataClass: \n(1) All method: For voice inputs, the user needs to refer to all \nor a select set of instances of certain objects. A static All \nmethod is provided that returns an array of all the instances \nof the DataClass. There are built-in functions t", "source": "database"}, "2_102": {"id": "2_102", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "hat support \nfiltering array elements based on value or value ranges, sort-\ning based on an order on a property, and extract an element \nby its index. \n(2) Get method: A static method that takes an id as input and \nreturns the instance with the corresponding id. \n(3) Current property: A static method that returns the instance \nthat is being referred to by the user. This is automatically \nannotated with GenieProperty. \nLike many of the common state management frameworks, when \nany of the properti", "source": "database"}, "2_103": {"id": "2_103", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "es of a DataClass instance are changed, UI com-\nponents on screen that refer to that property will be automatically \nre-rendered with the most up-to-date data. This ensures that the UI \nis always in sync with what is being represented in the state. \nReactGenie automatically maintains the instances of DataClass \nin memory to form the app’s state. DataClass can be backed up \nremotely, which is common in modern app development. \n3.3.3 Component/UI Code. ReactGenie developers need to define \nthe GUI", "source": "database"}, "2_104": {"id": "2_104", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " (as shown in the three code boxes in the bottom left of Fig-\nure 4) as a set of functional components3, similar with React. These \ncomponents can refer to each other to facilitate reuse. It is common \nfor every single instance of a DataClass or HelperClass to be \nrepresented by a component. For example, EmailView represents \nan Email instance, while SharedDocView represents a SharedDoc \ninstance. Therefore, ReactGenie introduces a special wrapper func-\ntion (or Component Wrapper for short) call", "source": "database"}, "2_105": {"id": "2_105", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ed GenieComponent \nto show that explicit mapping. Instead of the arbitrary parame-\nters of a normal functional component, components wrapped by \nGenieComponent take a DataClass or HelperClass instance as \ninput. GenieComponent allows the ReactGenie runtime to under-\nstand which component is mapped to which instance in memory \nto facilitate reference by touch. It also allows ReactGenie to render \nthe result of the user’s request using the developer-defined com-\nponent. While defining GenieCompone", "source": "database"}, "2_106": {"id": "2_106", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nt , the developer can also \nspecify an optional title and priority (both can be a method of the \nstate instance) for the interface, which is relevant for choosing the \ninterface to render multimodal command responses. \n3https://react.dev/learn/your-first-component 3.4 NLPL \nWe use a neural network to translate natural language commands \ninto NLPL, a domain-specific language (DSL) we created. We feed \nthe large-language model the automatically-extracted developer’s \nclass skeleton (only declarat", "source": "database"}, "2_107": {"id": "2_107", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ion and type information, no implemen-\ntations to save tokens, and reduce distractions), developer-supplied \nfew-shot examples, and the user’s current voice command and ask \nit to generate NLPL code for understanding the user’s intention. \nWe do not generate JavaScript directly because the expressive-\nness of JavaScript may cause unintended changes to the app’s be-\nhavior (contradicting F1). We cannot use a simple intent classifier, \nsuch as the one used by traditional voice assistants, because ", "source": "database"}, "2_108": {"id": "2_108", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "of the \ncomplexity of our user’s multimodal commands. The DSL interpreter \nmodule runs the generated NLPL code and calls the correspond-\ning methods in the developer’s state definition code (called state \ncode in the following sections). This also allows ReactGenie to han-\ndle users’ verbal references for simultaneous touch input through \nspecial reference functions (see Section 3.5.2). \nThe NLPL is designed to meet these language design goals: \nL1 Easy to generate : LLMs can generate syntactica", "source": "database"}, "2_109": {"id": "2_109", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "lly correct \nNLPL code. \nL2 Robust against generation errors: LLMs can generate seman-\ntically correct NLPL code. \nL3 Able to express multimodal commands : NLPL can express \ndiverse multimodal commands. \nTherefore, because of L1, NLPL has to be in a form similar to \nexisting programming languages that LLMs are trained on. To help \nwith L2, we also want NLPL to be strongly typed. We tried a syn-\ntax similar to TypeScript, but we noticed the LLM-based semantic \nparser occasionally generated the co", "source": "database"}, "2_110": {"id": "2_110", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "rrect parameters but in the \nwrong order. Therefore, we decided to use a syntax similar to Swift, \na strongly typed language that requires parameter names to be \nspecified in the function call. \nTo support the expressiveness of human languages L3, a simple \ndata formatting language such as JSON is not enough. JSON is \ngood at representing structured data and is sometimes used to \nrepresent simple intent with a single function call and parameters. \nHowever, the user’s commands can consist of mult", "source": "database"}, "2_111": {"id": "2_111", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "iple method \ninvocations and represent complex logic flow, so we looked at the \nlanguage structures of what people may say to a ReactGenie app. \nWhen interacting with a virtual assistant, people typically utter an \nimperative or interrogative sentence. \nAn imperative sentence must have a verb and an object. \nSome more complex imperative sentences can have object modi-\nfiers and verb modifiers. For example, “[Change the background \ncolor](verb) of [all the yellow](object modifier) [textboxes](obj", "source": "database"}, "2_112": {"id": "2_112", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ect) \n[to orange](verb modifier)”. Objects can be translated to func-\ntion calls to retrieve the corresponding objects, e.g., “this food” -\n>Food.Current() and “textboxes” ->TextBox.All() . Object mod-\nifiers can be translated to SQL-like operations, e.g., “food with \na rating above 3” ->Food.All().between(field:.rating,from \n:3), “all the yellow textboxes” ->TextBox.All().equals(field \n:.color,value:\"yellow\") . Note that we did not use SQL syn-\ntax because SQL does not have easy support for cal", "source": "database"}, "2_113": {"id": "2_113", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ling func-\ntions of objects, so it would have trouble translating verbs/ac-\ntions. Verb and verb modifiers are translated to function calls.  ReactGenie CHI ’24, May 11–16, 2024, Honolulu, HI, USA \nFor example, changing the background color to orange would be \n.setBackgroundColor(color:\"orange\"). \nWe also avoid supporting lambda expressions, variable decla-\nration, and control flow statements to avoid reference errors to \nincrease robustness ( L2) in translation and reduce complexity. For \nlambd", "source": "database"}, "2_114": {"id": "2_114", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "a expressions, NLPL automatically distributes function calls \nto individual elements of objects to support plural objects: “make \nall textboxes orange” ->TextBox.All().setBackgroundColor \n(color:\"orange\") . For variable declaration, we use method chain-\ning to avoid reference errors. This is because, in early testing, we \nfound that OpenAI Codex frequently refers to undeclared variables. \nPrior work on building a target language for a neural semantic \nparser also observed similar problems [18, 3", "source": "database"}, "2_115": {"id": "2_115", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "6]. For control flow \nstatements, we can use function call distributions for for-loops and \nuse the SQL-like syntax described above for if-statements. \nIn an interrogative sentence, the verb matters less (typically, the \nverb is just “is”), and there are still object and object modifiers. We \ncan translate the object and the object modifiers similarly: “When \nis [the last time](subject) [I ordered from this restaurant](subject \nmodifier)” ->Order.All().equals(field:.restaurant,value \n:Restaurant", "source": "database"}, "2_116": {"id": "2_116", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": ".Current()).sort(field:.date,ascending \n:false)[0].date. \nHuman languages are flexible and do not always have to follow \nthe exact grammar. Still, an LLM can automatically compose the \nNLPL features above to accommodate the user’s request as long as \nthe features are in the developer’s program. The full grammar of \nNLPL is listed in Appendix A. We implemented the DSL interpreter \nmodule using the peggy4 parser generator. \n3.5 System Workflow \nAs shown in Figure 4 right, ReactGenie provides libra", "source": "database"}, "2_117": {"id": "2_117", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ries that auto-\nmatically perform actions during two phases of the development \nand usage process: 1) transpilation and initialization time and 2) \nruntime. \nAt transpilation and initialization time (Figure 4 right top), Re-\nactGenie uses the annotations in the state code to generate LLM \nprompts containing class definitions and example parses for the \nsemantic parser and response generator modules (Section 3.5.1). \nReactGenie also reads the component wrapper code to determine \nthe mapping betwe", "source": "database"}, "2_118": {"id": "2_118", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "en the components and the state objects for the \ninput and output UI mapping modules (Section 3.5.2). \nAt runtime (Figure 4 right bottom), ReactGenie processes the \nuser’s multimodal voice and touch input. The voice part is trans-\nlated to NLPL using the semantic parser, and the touch points are \ntranslated to the referred state instances using the input UI map-\nping module. Knowing the NLPL and the referred state instances, \nthe NLPL interpreter can call the relevant methods and properties \nin ", "source": "database"}, "2_119": {"id": "2_119", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "the developer’s state code to achieve the user’s request. The \nNLPL interpreter records the final return value and the interme-\ndiary execution steps from executing each part of the composed \nmethod-chaining statement. ReactGenie further uses the final re-\nturn value to generate text feedback for the user. It also uses the \nexecution steps to graphically present the answer to a query-type \nrequest or the effect of an action-type request. \n4https://peggyjs.org/ Note that Transpilation is a source", "source": "database"}, "2_120": {"id": "2_120", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "-to-source translation pro-\ncess from the TypeScript the developer writes to Javascript that \nthe machine executes. Typically, a TypeScript app is transpiled to \nJavaScript to run in a mobile app or a browser. However, during \nthe transpilation process, the metadata, like typing and function \nparameter names, are removed. The metadata lost in transpilation \nis required by ReactGenie to create a large language model-based \nsemantic parser/response generator and the UI mapping. Therefore, \nwe buil", "source": "database"}, "2_121": {"id": "2_121", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "t our transpliation plugins to use the developer’s code be-\nfore transpilation to generate the modules that ReactGenie uses at \nruntime. \n3.5.1 Transpilation and Initialization for State Code. ReactGe-\nnie uses a custom transpiler plugin that generates extra meta-\ndata for @GenieProperty , and @GenieFunction of DataClass and \nHelperClass. \nWe use in-context learning to implement the semantic parser \nand the response generator. During initialization of the app, Re-\nactGenie will load injected met", "source": "database"}, "2_122": {"id": "2_122", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "adata from state classes (classes in \nstate code) to generate a base prompt shared by both the semantic \nparser and the response generator. LLMs work by generating text \ncontinuations given a paragraph of previous text. The provided pre-\nvious text is often called the prompt. By controlling the prompt, we \nchange the information the LLM has access to and guide the LLM \nto do what we want (generate the corresponding NLPL of the user’s \ncommand). ReactGenie’s generated prompt contains two parts: 1", "source": "database"}, "2_123": {"id": "2_123", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": ") \nThe class definitions contain all the DataClass and HelperClass \nmethod and property definitions with the implementation stripped \nout. It is rendered in a format similar to Swift syntax. 2) The exam-\nple parses provided by the developer are also included as few-shot \nexamples. \nThe user input is then appended to the generated prompt and \nused in the LLM-based semantic parser for NLPL translation. The \nresponse generator prompts the LLM with the generated prompt, \nthe user input, the parsed N", "source": "database"}, "2_124": {"id": "2_124", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "LPL, and the description of the return \nvalue from the execution of NLPL to produce a short text response. \nWe built the semantic parser using the OpenAI Codex model \ncode-davanci-2 and the response generator using the OpenAI \nGPT 3.5 model text-davanci-3. \n3.5.2 Transpilation and Initialization for UI Code. At initialization \ntime, we also process GenieComponent functions to save a map-\nping between the GenieComponent and the GenieClass that they \nare representing. We generate input and output ", "source": "database"}, "2_125": {"id": "2_125", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "UI mapping mod-\nules from this information. We monitor the bounding box of all \nGenieComponent s for input mapping. When the user touches the \nscreen while expressing a multimodal command, ReactGenie will \nuse the bounding box information to determine which component \nthe user is pointing to. \nIt is common for multiple UI components to cover the area \nwhere the user taps on the screen. For example, in Figure 1, all \nthe FoodThumbnail components overlap with the OrderItemView \ncomponents. ReactGe", "source": "database"}, "2_126": {"id": "2_126", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nie allows the user to use their voice to disam-\nbiguate the reference: If the user mentions food, such as “this food” \n(FoodItem.Current() ), or actions that can only be done with food, \nlike “what is the price for this” (FoodItem.Current().price ), Re-\nactGenie will use the FoodItem object and vice versa. In the special  CHI ’24, May 11–16, 2024, Honolulu, HI, USA Yang et al. \ncase where multiple components of the same type cover the tapped \narea, ReactGenie uses the one with the smallest boun", "source": "database"}, "2_127": {"id": "2_127", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ding box. \nAnother common scenario is that if one object is clearly in the \n“foreground” of the graphical UI, the user may naturally refer to \nit as “this” without explicitly specifying the component via touch. \nSo, when the user refers to a state class and either there is no \ntouch point, or the touch point does not match any component \nrepresenting that class, ReactGenie will use the largest component \non the screen representing that class as the reference. \nWe also use GenieComponent to gener", "source": "database"}, "2_128": {"id": "2_128", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ate output UI mapping \nmodules. We gather all the GenieComponent s with supplied priority \nand title and group them by the state class they represent. When \nthe result of the executed NLPL is a state class instance, ReactGenie \nenumerates through all the GenieComponent s representing that \nclass and renders the one with the highest priority. \nThere are two types of execution results. The first type is for \nquery-type requests where the translated NLPL returns an instance \nthat can be rendered by", "source": "database"}, "2_129": {"id": "2_129", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " a GenieComponent . This is common when \nthe user asks to either retrieve some data “what are my most recent \norders from this restaurant?” or to perform some action with a \nclear result “what vegetarian food does this restaurant offer?”. In \nthat case, rendering the result on the screen would be intuitive. So, \nwhen the return value can be represented by a GenieComponent , \nReactGenie will always find the highest priority GenieComponent \nand render it. \nThe second type is for action-type reques", "source": "database"}, "2_130": {"id": "2_130", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ts where the trans-\nlated NLPL returns a value that cannot be rendered by a \nGenieComponent . For example, if the user asks to “add a ham-\nburger to the cart” (NLPL: Order.GetActiveCart().addItem( \n[Food.Named(\"hamburger\")]) , it would return void which can-\nnot be rendered. For these actions, the return value is less important, \nand the user is more interested in the side effects of the action (e.g., \nthe cart has been updated). In that case, ReactGenie will back trace \nthe chained execution st", "source": "database"}, "2_131": {"id": "2_131", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "eps and find the last renderable result is the \nreturn value of Order.GetActiveCart() (i.e., an instance of a cart). \nReactGenie will also check all the currently visible components \nto see if there is any that already represent the same instance of \nthat cart on screen. ReactGenie would only render this result if the \ncurrent page has no component representing the same instance. \nFor example, when the user is already on a restaurant page where \nthey can see an indicator of the number of items i", "source": "database"}, "2_132": {"id": "2_132", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "n the cart (e.g., \nthe cart icon with a counter also represents the cart instance), it \nwould be redundant to show the cart again. However, if the user is \non the past order page where they cannot see any representation \nof the cart, it would be useful to show the cart to ensure the user \nunderstands the action being performed. \n3.5.3 Runtime. Like normal React or React-Native apps, when \nusers interact with buttons and visual controls, the app calls the \ncorresponding methods to update data in ", "source": "database"}, "2_133": {"id": "2_133", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "the state instances. In \nturn, the state instances trigger the GenieComponent s to update \ntheir UI. \nAs shown in the bottom right of Figure 4, the multimodal in-\nteractions are handled through developer modules (Section 3.3), \nthe NLPL modules (Section 3.4), and the generated modules (Sec-\ntion 3.5.1), collectively. When the user touches the microphone \nbutton on the UI, ReactGenie starts listening to the user’s voice command and intercepts all touch events on the screen. From this, \nwe gather ", "source": "database"}, "2_134": {"id": "2_134", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "two inputs: the user’s voice command and the touch \npoint(s). We use speech recognition from Azure to transcribe the \nuser’s speech to text. The voice command transcript is then passed \nto the semantic parser module to generate the NLPL code. The \ntouch point(s) are passed to the input UI mapping module to de-\ntermine which component and state instance the user can refer \nto. Both pieces of information are then passed to the NLPL inter-\npreter to execute the NLPL code with the corresponding rele", "source": "database"}, "2_135": {"id": "2_135", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "vant \nstate instance. ReactGenie uses the methods and properties of the \ndeveloper-provided state classes to execute the NLPL code. After \nthe execution, we record both the final return value and the inter-\nmediate values during execution. ReactGenie uses the return value \nand the parsed DSL to generate a text response using the response \ngenerator. ReactGenie also passes execution steps to the output UI \nmapping module to determine whether and how to render the result \non the screen. Finally, t", "source": "database"}, "2_136": {"id": "2_136", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "he text response and the rendered UI are \nused to generate Feedback in Text and Content in UI. \n4 FRAMEWORK EVALUATION \nWe first evaluate the development framework by checking whether \nour design goals have been reached. \nF-RQ1 How expressive is the ReactGenie framework? (F1) \nF-RQ2 How much time is needed for expert developers to develop \nmultimodal apps using ReactGenie compared with existing \nframeworks? (F2) \nF-RQ3 How easy is it to learn and use ReactGenie to develop \nmultimodal apps for no", "source": "database"}, "2_137": {"id": "2_137", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "vice developers ? (F3) \n4.1 Expressiveness of the Framework \nTo answer whether ReactGenie can support the expressiveness of \nmobile apps (F-RQ1), we built three example apps across three \nmajor categories of apps: food & drink, social networking, and \nbusiness in different interface styles, as shown in Figure 5. The \nimplementation statistics are shown in Table 2. \n4.1.1 ReactGenieFoodOrdering. ReactGenieFoodOrdering is a food \nordering app that allows users to order food from a restaurant. It \n", "source": "database"}, "2_138": {"id": "2_138", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "allows users to browse menus, manage shopping carts, and check \norder history. For example, users can say “Reorder my last order”, \nclick on a food item and say “Add three of this to my cart”, or click on \nthe restaurant and say “Show me the menu of this restaurant”. The \napp is comprised of 2689 lines of code, with only 88 (3%) related to \nbuilding the multimodal UI. Note that every example parse provided \nby the developer takes four lines of code, and every GenieClass , \nGenieFunction , and Ge", "source": "database"}, "2_139": {"id": "2_139", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nieProperty annotation takes just one line \nof code. \n4.1.2 ReactGenieSocial. ReactGenieSocial is a social networking \napp that allows users to post pictures, comment on pictures, and \nshare pictures with friends. It allows users to browse, interact with, \nand share posts. For example, users can say “Show me posts from \nJohn”, “Can you show me posts from Mark that have been liked \nbefore?”, or click on the screen and say “Share this post with Emma”. \nThe app is comprised of 1034 lines of code, w", "source": "database"}, "2_140": {"id": "2_140", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ith only 49 (5%) related \nto building the multimodal UI.  ReactGenie CHI ’24, May 11–16, 2024, Honolulu, HI, USA \nReactGenieFoodOrdering ReactGenieSocial ReactGenieSignReactGenieSign - NDA Management\nReactGenieSocial\nFigure 5: Example apps built with ReactGenie. Left: ReactGenieFoodOrdering, a food ordering app. Middle: ReactGenieSocial, \na social networking app. Right: ReactGenieSign, a business app for distributing and collecting signed NDAs. \nApp Name FoodOrdering Social Sign \nDataClass \nHelp", "source": "database"}, "2_141": {"id": "2_141", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "erClass \nGenieComponent \nOtherComponents GenieFunction GenieProperty State Code (lines) \nComponent Code (lines) \nExamples (count) Order, FoodItem, Restaurant \nOrderItem \n11 \n8 22 18 835 \n1854 \n11 Post, User, Message \n7 2 8 10 449 \n585 \n6 User, Document, SignatureRequest \nEmailAddress \n6 \n3 11 16 421 \n446 \n6 \nTable 2: Implementation statistics for demo apps. We listed all the DataClass , HelperClass , and the number of GenieComponent \nand GenieFunction used in the apps. We also listed the number ", "source": "database"}, "2_142": {"id": "2_142", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "of lines of code for the state and component code and the \nnumber of example parses provided for the voice parser. \n4.1.3 ReactGenieSign. ReactGenieSign is a business app that man-\nages NDAs and contracts. It allows users to create documents, share \ndocuments with clients for signing, and manage clients. For exam-\nple, users can say “Show me the signature request from John”, click \non the email address and say “Only show me requests from this \nemail”, or click on the email address and say “Share", "source": "database"}, "2_143": {"id": "2_143", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " the document \nin the most recent signature request to this email”. The app is com-\nprised of 867 lines of code, with only 51 (6%) related to building the \nmultimodal UI. \n4.1.4 Summary. Implementing the three apps in distinct domains \nthat support a wide range of multimodal commands demonstrates the expressiveness of ReactGenie. While building these demo apps, \nwe noticed that different UIs are naturally decomposed into com-\nponents that represent different ReactGenie state instances, which \nma", "source": "database"}, "2_144": {"id": "2_144", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "de it easy to decompose the UI into GenieComponent s. We were \nalso able to lay out the graphical UI in the most appropriate way \nand then decompose the UI layout into individual components rep-\nresenting different state classes for UI mapping purposes. We also \nnoticed that only a small fraction (5% on average) of the code must \nbe written to handle multimodal interactions. This is particularly \nimpressive since defining multimodal interactions can be intricate \nand typically requires substanti", "source": "database"}, "2_145": {"id": "2_145", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "al code to support.  CHI ’24, May 11–16, 2024, Honolulu, HI, USA Yang et al. \n4.2 Development Time: Expert Evaluation \nTo evaluate the development time required for ReactGenie apps \n(F-RQ1), we conducted an expert evaluation comparing the time \nto build apps with ReactGenie to the time required for building \nsimilar features with baseline tools. \n4.2.1 Study Design. There is no readily available multimodal frame-\nwork that adds multimodal capabilities to a multimodal app that has \ncomparable cap", "source": "database"}, "2_146": {"id": "2_146", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "abilities to ReactGenie. Therefore, we used GPT-3 \nfunction calling5 and React to implement the baseline app. GPT-3 \nfunction calling is a service supported by large language models \nthat can convert natural language into function calls. We chose it as \nthe baseline because a developer can use it to translate users’ voice \ncommands into function calls defined in the app, which makes it \nthe closest off-the-shelf solution to help with the multimodal app \ndevelopment tasks that ReactGenie supports", "source": "database"}, "2_147": {"id": "2_147", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": ". \nFor the GPT-3 function-calling condition, we used a typical React-\nRedux architecture where there is no object-oriented state abstrac-\ntion and the UI state is stored in a monolithic data store. For example, \nin the function-calling condition, the developer would implement \nan action on the monolithic data store called RATE_FOOD(food_-\nname:string,rating:number) and pass the same function to the \nGPT-3.5-turbo endpoint as a function candidate. When the user says \n“Rate the hamburger five star", "source": "database"}, "2_148": {"id": "2_148", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "”, GPT returns a function call action \nthat it wants to call RATE_FOOD(hamburger,5) , and the developer \ncan execute the function for the monolithic state. \nIn the study, we asked an expert user6 of ReactGenie to build a \nmultimodal timer app (ReactGenieTimer, Figure 6 right). The timer \napp allows users to create timers and start/stop timers with voice \nand touch. \nWe asked the developer to first write the part of the app that \nis agnostic to ReactGenie (the boilerplate code), such as the React", "source": "database"}, "2_149": {"id": "2_149", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " \nUI code, CSS files, and basic configurations. From there, we timed \nhow long the expert developer finished the multimodal app with \nReactGenie. From the same starting point, we also timed how long \nthe expert developer could implement similar features with GPT-3 \nfunction calling. \n4.2.2 Results. The implementation of both versions of the app \nstarts with a boilerplate project that contains the framework-\nagnostic part of the code (337 lines of code). The expert developer \ntook 45 minutes to c", "source": "database"}, "2_150": {"id": "2_150", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "omplete the ReactGenie multimodal app and \nadded 159 lines of code to complete the app. In comparison, it took \nthe same developer 177 minutes to implement similar functionality \nwith GPT-3 function calling and an additional 523 lines of code. \nWithin that period of time, 52 minutes and 166 lines of code were \nspent on implementing the basic react-redux-based state manage-\nment. \nThe additional code and time for GPT-3 function calling is due \nto the following: \n(1) The developer needs to provide", "source": "database"}, "2_151": {"id": "2_151", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " the function signature \nto the model manually , and when the model thinks a \nfunction call is necessary, the developer needs to call the \ncorresponding function call. \n5https://openai.com/blog/function-calling-and-other-api-updates \n6The expert developer was the first author of this paper. (2) The developer needs to build extra convenient func-\ntions for GPT-3. Since GPT-3 can only call one function at a \ntime, it has trouble executing actions like “Start the exercise \ntimer” because that invol", "source": "database"}, "2_152": {"id": "2_152", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ves two steps: 1) retrieve the timer \ncalled exercise, and 2) start the retrieved timer. For GPT-3 \nto work, the developer has to implement a function that can \nstart a timer by its name. \nEven with the additional lines of code, the GPT-3 function calling \nversion lacks a few significant benefits of the ReactGenie version: \n(1) It cannot support references by touch due to a lack of the \nUI mapping module. For example, it cannot support “Start \nthis timer.” while tapping on a timer. \n(2) It canno", "source": "database"}, "2_153": {"id": "2_153", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "t support rich commands unless explicitly pro-\nvided by the app developer. For example, it cannot support \n“Pause all timers with less than two minutes left.” \n(3) It cannot navigate to the relevant page after executing a \ncommand due to the lack of UI mapping. For example, when \nthe user says “Start the cooking timer” while the cooking \ntimer is not currently on the page, the app will not show the \nuser the cooking timer visually. \nThe 3.9x time used and 3.3x lines of code to implement similar ", "source": "database"}, "2_154": {"id": "2_154", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "\nfeatures in the baseline condition show that ReactGenie drastically \nreduced the implementation time for developers to build multi-\nmodal apps. In addition, the missing features in the GPT version \nalso demonstrate the usefulness of ReactGenie for expressive mul-\ntimodal app development. \n4.3 Usability and Learnability: Developer \nStudies \nTo evaluate the usability and learnability of ReactGenie (F-RQ3), \nwe conducted an IRB-approved user study asking novice developers \nto build multimodal appl", "source": "database"}, "2_155": {"id": "2_155", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ications with our framework. Considering \nthat developers cannot complete multimodal applications within \nan acceptable time frame through direct API calls, our study was \nfocused on the framework-specific part of the implementation to \nmeasure the usability of ReactGenie for building multimodal apps. \nWe also evaluated developers’ comprehension of the framework. \n4.3.1 Study Design. The study was facilitated using a remote desk-\ntop to ensure all participants completed the coding tasks in the \n", "source": "database"}, "2_156": {"id": "2_156", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "same environment. During the study, the experimenter introduced \nthe study goals and explained study-related concepts such as multi-\nmodal apps since most participants did not have multimodal devel-\nopment experience. Then, the experimenter helped the developer \nconnect to the remote experimental environment via video confer-\nencing software. \nThe main study process contains two parts: the first, where devel-\nopers learned ReactGenie and the second, where they built an app \nin ReactGenie. In the", "source": "database"}, "2_157": {"id": "2_157", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " first part, participants familiarized themselves \nwith ReactGenie by creating a multimodal counter application (Re-\nactGenieCounter, Figure 6 left) guided by a tutorial. In the second \npart, participants were asked to leverage their knowledge from \nthe example counter app development to construct a multimodal \ntimer application (the same ReactGenieTimer app described in Sec-\ntion 4.2) independently. To make it feasible to complete the tasks \nwithin an acceptable amount of time and to ensure tha", "source": "database"}, "2_158": {"id": "2_158", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "t participants  ReactGenie CHI ’24, May 11–16, 2024, Honolulu, HI, USA \nReactGenieTimer\n ReactGenieCounter\nFigure 6: The ReactGenieCounter and ReactGenieTimer apps built in the developer study. The ReactGenieCounter app allows \nusers to create counters and increment/decrement counters. The ReactGenieTimer app allows users to create/edit/delete timers \nand start/stop timers. Both apps support a variety of multimodal commands. \ncould focus on using the ReactGenie framework to implement mul-\ntimoda", "source": "database"}, "2_159": {"id": "2_159", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "l features, we provided boilerplate code with the basic GUI \nimplementation (similar to what was built by the expert developer \nin Section 4.2 ) for the two development tasks. \nDuring the study, participants were required to complete a React-\nGenie comprehension quiz and a post-study survey that recorded \ntheir demographic information and asked them to fill out the SUS \nusability scale [8] and the NASA-TLX cognitive load scale [31]. \nFinally, we have a short interview about their experience usin", "source": "database"}, "2_160": {"id": "2_160", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "g \nthe framework. We recorded the audio and the developer’s screen \nduring the entire process with the user’s permission. Each user \nreceived a $60 gift card as compensation after the study. \n4.3.2 Participants. We recruited developers with React develop-\nment experience to ensure they could handle the non-multimodal-\nrelated coding tasks beyond what ReactGenie is designed for. There-\nfore, we designed a quiz with seven programming questions and \ndeployed a recruitment screener. These questions ", "source": "database"}, "2_161": {"id": "2_161", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "cover basic React \nskills (such as how to use a React component, and how to manipu-\nlate state in React) and basic object-oriented programming skills \n(e.g., what is this pointer, and how to write class constructors). \nOnly developers who answered at least five of the seven questions \ncorrectly were invited to participate in the study. \nWe recruited 12 participants (10 males and two females) in the re-\nmote user study by distributing the recruitment screener link using \na convenience sample. Our", "source": "database"}, "2_162": {"id": "2_162", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " participants include student developers \nand professional developers with an average age of 23.8 (f = 3.43). \nAll participants had React development experience; ten developers also had TypeScript development experience. The most experienced \ndeveloper had seven years of React development experience. \n4.3.3 Results. All participants successfully built the applications \nwithin 150 minutes. The average completion time was 109.7 minutes \n(f = 24.19), with 42.3 minutes (f = 14.90) for the first phas", "source": "database"}, "2_163": {"id": "2_163", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e and \n67.3 minutes (f = 17.52) for the second phase. This shows that devel-\nopers with React and object-oriented programming experience can \nquickly learn and use the ReactGenie framework, illustrating the \nframework’s ease of learning and high usability. The user with the \nshortest task completion time needed only 82 minutes to complete \nboth tasks. \nAlmost all participants (11/12) answered the seven post-task quiz \nquestions correctly. A single participant got one question wrong re-\nlated to ", "source": "database"}, "2_164": {"id": "2_164", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "predefining the interface. This shows that developers could \ndevelop a multimodal app and correctly understand how the Re-\nactGenie code they wrote corresponds to the multimodal features, \nwhich suggests the design of the ReactGenie framework is easy \nto understand. The main parts that participants found relatively \ndifficult to understand are the GUI declarations and providing exam-\nple parses. Participants said it took them more time to understand \nthe GenieComponent function and how to bind d", "source": "database"}, "2_165": {"id": "2_165", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ata types to the \ncorresponding interfaces. Regarding example editing, participants \nthought it was hard to determine what should be included in the \nexamples. Although participants found these parts difficult to un-\nderstand, they could use ReactGenie smoothly after learning and \ntesting their apps. Participant 11 said, “ReactGenie incorporates  CHI ’24, May 11–16, 2024, Honolulu, HI, USA Yang et al. \nnatural language to UI building, and it is not hard to do that when \nprogramming.” \nWe evaluat", "source": "database"}, "2_166": {"id": "2_166", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ed ReactGenie’s usability using a part of the seven-\npoint SUS scale. ReactGenie received high SUS scores in terms of \nease of use (` = 6.16, f = 0.58), enjoyable (` = 6.08, f = 0.79), \nintuitive (` = 6.08, f = 0.90), and willing to use (` = 6.08, f = 0.79). \nParticipant 10 says, “(ReactGenie was) easy to pick up if you know \nReact.” At the end of the experiment, participant 12 commented \nthat he enjoyed programming with ReactGenie and asked if there \nwas any library he could use to access React", "source": "database"}, "2_167": {"id": "2_167", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "Genie in his daily \nprogramming. All participants agreed that ReactGenie is easy to \nuse (medium 6 out of 7 points Likert scale), and all participants were \nwilling to use ReactGenie to build real-life multimodal applications \n(medium 6 out of 7 points Likert scale). The average NASA-TLX \nscore for the overall study was 21.99 out of 100 (the lower, the \nbetter), showing the practicality and low usage burden to program \nwith ReactGenie. \n5 INTERACTION EVALUATION \nWe evaluated the interaction prov", "source": "database"}, "2_168": {"id": "2_168", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ided by the ReactGenie frame-\nwork through two aspects: the performance of the generated neural \nsemantic parser and the overall user experience of a ReactGenie \napp. \n5.1 Parser Performance \nTo understand how well the ReactGenie parser works with in-\nformation extracted from the developer’s code, we elicited com-\nmands from crowd workers for the ReactGenieFoodOrdering app \nand tested our parser. Specifically, we would like to know: \nRQ1 What percentage of the commands 1) is achievable with a \ns", "source": "database"}, "2_169": {"id": "2_169", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ingle UI interaction on screen, 2) fall into the three targeted \ninteractions mentioned in Section 3.1, or 3) are out of scope \nof ReactGenie. \nRQ2 How accurate the parser is when parsing commands in the \ntargeted interactions. \n5.1.1 Elicitation. We wanted to obtain multimodal commands that \nusers may use in a real-world scenario. We adopted a similar method \nas described as Cloudlicit [11]. We provided the user with three \nscreenshots (restaurant listing page, restaurant menu page, and \npast o", "source": "database"}, "2_170": {"id": "2_170", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "rders page) of the US’s two most popular food ordering apps: \nDoorDash and UberEats. In our pilot study, we found that many \nparticipants’ thoughts on what they can do in these apps are limited \nto what’s on-screen and what they think the current generation \nof voice assistants can do. Therefore, we showed the final study \nparticipants 12 videos randomly, containing four videos for each of \nthe three categories of interactions being executed on a different \napp (home page of the Apple app store)", "source": "database"}, "2_171": {"id": "2_171", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": ". Among these 12 videos, we \nalso ensured half involved only voice and the other half contained \nvoice and touch. \nWe recruited 50 participants from Prolific, a crowdsourcing plat-\nform. We used the balanced sample options when finding partici-\npants, so we had 25 female and 25 male participants. The age range \nof the participants was 20 to 79, with a median age of 29. The \nsurvey took approximately five minutes, and we paid $2 for each \nparticipant. From these 50 participants, we obtained 300 c", "source": "database"}, "2_172": {"id": "2_172", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ommands. We \nfiltered out 12 unclear or unrelated responses to the survey. For \nexample, one participant wrote “various good foods to order or view \nthat can be good” as a command. After filtering, 288 commands \nremained in the dataset. \n5.1.2 RQ1: Percentages of categories of commands. We classify the \ncommands into three categories: \n(1)\n Simple UI interaction : The command can be achieved with \na single UI interaction on screen. For example, “Look at \nthe Curry Up Now Menu” when the restauran", "source": "database"}, "2_173": {"id": "2_173", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "t is visible on \nscreen. \n(2) Within the three targeted interaction categories : The \ncommand falls into the three targeted interactions mentioned \nin Section 3.1. For example, one participant wrote “Order me \ntwo big macs and large fries from Mcdonald’s for pickup.” With \na GUI, this command would typically be achieved via multi-\nple taps to find the restaurant, add the foods, and configure \nthe delivery options. \n(3) Out of scope of ReactGenie: The command is out of scope \nof ReactGenie. For e", "source": "database"}, "2_174": {"id": "2_174", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "xample, “How do I repeat past orders?”. \nReactGenie tries to help people complete complex tasks, but \nit does not have built-in knowledge about how to use the UI \nof the app. \nTwo researchers collaboratively labeled 30 commands to get a \nrubric for the rest of the commands. They then labeled the rest of \nthe commands (258 commands) using the rubric separately. Both la-\nbelers labeled the same label for 224 commands and different labels \nfor 34 commands. Because the labels have a skewed distribut", "source": "database"}, "2_175": {"id": "2_175", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ion, we \nused Gwet’s AC1 [30] to measure the inter-rater reliability. The AC1 \nscore is 0.83, which means the labels are highly consistent. They \nresolved the disagreement and got a final label for each command. \nFrom this analysis, we found that 100 of the elicited commands \nwere simple UI interactions, 172 commands fell into the three tar-\ngeted interactions, and 16 commands were out of the scope of \nReactGenie. \nThis shows that users can come up with tasks beyond just simple \nUI interactions ", "source": "database"}, "2_176": {"id": "2_176", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "even when the type of multimodal interfaces that \nReactGenie supports are not available in commercial apps. It may \nalso hint at user interest in the types of interactions that we propose \nhere. \n5.1.3 RQ2: Accuracy of the parser. We tested the parser on the \n172 commands that fall into the three targeted interactions. We \nran the parser based on the ReactGenieFoodOrdering app and read \nthe generated NLPL to see if the parses are correct. While work-\ning on labeling the correctness, we also noti", "source": "database"}, "2_177": {"id": "2_177", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ced that many of the \ncommands are not supported by our simple demo app, e.g., ReactGe-\nnieFoodOrdering only knows delivery fees for different restaurants, \nbut not estimated delivery times. So we also labeled whether the \nfeature that the command tries to use is supported by ReactGe-\nnieFoodOrdering. \nOur analysis showed that 101 commands are supported by Re-\nactGenieFoodOrdering, and 71 commands are not supported. Some \nfeatures that are missing from ReactGenieFoodOrdering are 1) top-\npings/cu", "source": "database"}, "2_178": {"id": "2_178", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "stomization of a food item; 2) reviews of a restaurant or a \nfood item; and 3) delivery time estimates for restaurants.  ReactGenie CHI ’24, May 11–16, 2024, Honolulu, HI, USA \nFrom the 101 commands supported by ReactGenieFoodOrdering, \nwe found that 91 commands are parsed correctly by the parser, and \nten are not. Therefore, on this dataset, the parser has an accuracy \nof 90%. A parser accuracy of 90% should be considered very high \ncompared to prior work on neural semantic parsers’ accuracy on", "source": "database"}, "2_179": {"id": "2_179", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " \ncompound commands [19]. Seven of the ten incorrect parses would \nresult in syntax errors (such as used .first() rather than NLPL \nsupported [0]) or runtime errors (such as ordering the last meal \nfrom “A” has been translated to order the food called “A”, thus the \nsystem will not be able to find “A” as a food). Three of the incorrect \nparses would be helpful but not give the desired behavior. For \nexample, “Find me the closest pizza restaurant” was translated to \n“find the closest restaurant”.", "source": "database"}, "2_180": {"id": "2_180", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " None of the errors resulted in behavior \ncompletely different from the user’s expectations. \nWe also looked at the 71 commands that are not supported \nby ReactGenieFoodOrdering. These commands mention fea-\ntures that are not in the ReactGenieFoodOrdering app. To \nour surprise, the parser also generated sensible NLPL for \nthe majority (38) of these commands. The ReactGenie parser \napproximates the request command with available features \nin the app for 24 of these commands. For example, React-\nG", "source": "database"}, "2_181": {"id": "2_181", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "enie parser generates Restaurant.GetRestaurant(name \n:\"pizzahut\").getFoodItems().between(field:.price,from \n:0,to:5) for the command “What deals does pizza hut have?”. In \nthis case, the parser approximates deals with food items that are less \nthan 5 USD. For 14 commands, the parser would generate function \ncalls and property accesses that are not supported by the app. For \nexample, the parser parsers “What time does Chipotle open?” to \nRestaurant.GetRestaurant(name:\"Chipotle\").openingTime . \nIn", "source": "database"}, "2_182": {"id": "2_182", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " this case, ReactGenieFoodOrdering does not have the property \nopeningTime for restaurants, but the parser is still capable enough \nto generate a sensible NLPL. In the future, the ReactGenie runtime \ncan leverage this information to inform the user of the missing \nproperty and potentially even suggest the developer add common \nmissing features to the app. \nThere were 33 unsupported commands that were not parsed cor-\nrectly by the parser. Some of them are due to the parser generating \nungrammatic", "source": "database"}, "2_183": {"id": "2_183", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "al NLPL, and others use incorrect properties and meth-\nods. For example, the parser parses “Find restaurants that deliver \nin less than 25 minutes.” to Restaurant.All().matching(field \n:.deliveryFee,value:<25) . In this case, ReactGenieFoodOrder-\ning does not know the estimated delivery time of restaurants, but \nthe correct parsing should be Restaurant.All().between(field \n:.deliveryTime,from:0,to:25). \nThe results show that ReactGenie parser is a reasonably good \nimplementation for parsing natu", "source": "database"}, "2_184": {"id": "2_184", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ral language commands to NLPL \nusing only information extracted from the shared logic code and \nthe few-shot examples provided by the developer. \nAnother interesting metric is that 104 of the 172 commands \ncontain at least one touch point, but there are only 18 cases where \nthese touch points are required to execute the command. In many of \nthese commands, the user taps relevant objects, hoping that it would \nhelp the system understand. For example, they would tap on the \n“Restaurant” menu bar w", "source": "database"}, "2_185": {"id": "2_185", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "hile saying “Show me a pizza restaurant \nnear me.” Another interesting observation is that when they referred \nto objects on screen, they often would not use a reference term like \n“this” or “that.” Instead of saying “Reorder this order”, the participant would say “Reorder my Mendocino Farms order from Thursday.” This \nshows a potential opportunity to improve the semantic parser by \nalways adding the touch context even when it seems unnecessary. \n5.2 Usability of Supported Interactions \nWe condu", "source": "database"}, "2_186": {"id": "2_186", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "cted a usability study with the ReactGenieFoodOrdering \napp to understand if the generated multimodal UIs are useful for \nend users. We measured the performance of the multimodal UIs \nin terms of the time it takes to complete a task, the cognitive load, \nand the usability of the experience when using the app compared \nto the same app limited to using only the GUI. \n5.2.1 Study Design. In the study, we asked participants to complete \na set of tasks using two variants of the ReactGenieFoodOrdering", "source": "database"}, "2_187": {"id": "2_187", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " app, \none generated by ReactGenie and one limited to only the GUI. We \nused a within-subjects design, where each participant completed the \nsame tasks using both variants of the app. For each app variant, we \nfirst teach the participant how to use the app using one training task. \nSpecifically, for the ReactGenie condition of the app, we explained \nthat in addition to typical touch-only interaction, they can also \ntap the microphone button to initiate a speech + gesture command \nwhen they want ", "source": "database"}, "2_188": {"id": "2_188", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "to. We then asked them to complete two test tasks \nwith the variant. After completing the two tasks, we asked them \nto complete a survey about their cognitive load using the system \n(using NASA-TLX [31]) and the usability of the experience (using \nSUS [8]). At the end of the study, we asked the participants about \ntheir subjective preferences between the two variants of the app \nand their reasons for their preferences. \nWe designed one training task and two test tasks for each variant \nof the ap", "source": "database"}, "2_189": {"id": "2_189", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "p. The training tasks are to order the cheapest food item \nfrom the menu of two different restaurants. The test tasks are re-\nordering an order from two different days (today or yesterday), and \nfinding the most recent order containing two different items. When \npresenting these tasks, we described a scenario, what we wanted \nthem to do, and the expected outcome (order placed screen or a \ncertain screen showing a past history order). We counterbalanced \nthe order of the two apps and the order of", "source": "database"}, "2_190": {"id": "2_190", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " the three tasks. \n5.2.2 Participants. We recruited 16 participants, aged 18–30, with \na median age of 23. Eight of our participants are female, six are male, \none stated other, and one prefers not to say. One of our participants \nuses food ordering daily, two use it weekly, five use it monthly, \nseven use it a few times per year, and one rarely or never uses it. \nAll of our participants use graphical mobile interfaces daily. Two \nof our participants use voice interfaces daily, four use them wee", "source": "database"}, "2_191": {"id": "2_191", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "kly, \ntwo use them monthly, four use them a few times per year, and \nfour rarely or never use them. The study took about 30 minutes \nto complete, and we compensated each participant with a 15 USD \nAmazon gift card for their time. \n5.2.3 Results. We computed the time it took to complete each task \nusing the graphical UI and the multimodal UI (see Figure 7). The \naverage time it took to complete each task using the graphical \nUI was 63.6 seconds, while the average time it took to complete \neach ta", "source": "database"}, "2_192": {"id": "2_192", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "sk using the multimodal UI was 33.6 seconds. We used a \npaired t-test and found that the difference is statistically significant \n(? = 0.0004 , C = 3.955).  CHI ’24, May 11–16, 2024, Honolulu, HI, USA Yang et al. \nGUI-only ReactGenie\nCondition20406080100120Time (seconds)\n***Experiment Time Comparison between MMI and GUI\nFigure 7: Users can complete common tasks faster (? = \n0.0004 , C = 3.955) with the multimodal interface (MMI) app \nbuilt with ReactGenie compared with a baseline GUI app. \nWe co", "source": "database"}, "2_193": {"id": "2_193", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "mpared NASA-TLX average scores between the two condi-\ntions (see Figure 8 left). The average NASA-TLX score for the graphi-\ncal UI is 34.5, while the average NASA-TLX score for the multimodal \nUI is 24.6 (note: lower is better). We used a Wilcoxon test and found \nthat the difference is statistically significant ( ? = 0.013, I = 21). \nWe compared the average SUS scores between the two conditions \n(see Figure 8 right). The average SUS score for the graphical UI is \n63.3, while the average SUS scor", "source": "database"}, "2_194": {"id": "2_194", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e for the multimodal UI is 73.0 \n(note: higher is better). We used a Wilcoxon test and found that the \ndifference is statistically significant ( ? = 0.031, I = 22). \n11 out of 16 of our participants preferred the ReactGenie generated \nmultimodal UI over the graphical UI. For participants who preferred \nthe multimodal UI, the most common reason was that it was easier \nto use (P4, P8, P13, P16). P2 mentioned that they would prefer to \nuse a mix of both in the real world, which is well supported by", "source": "database"}, "2_195": {"id": "2_195", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " Re-\nactGenie. P6 mentioned that the multimodal UI could be especially \nuseful when they are unfamiliar with the app. P12 mentioned that \nthe multimodal commands allowed them to do more complex tasks \nwith a clear path rather than searching and finding out how to do so \nin the graphical UI. For participants who preferred the graphical UI, \nthe most common reason was that the speech recognition was not \naccurate (P5, P7, P14). P9 and P11 mentioned that they generally \ndo not use voice interfaces.", "source": "database"}, "2_196": {"id": "2_196", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " \n5.2.4 Discussion. The results of our usability study show that the \nmultimodal UIs generated by ReactGenie are more efficient, have \na lower cognitive load, and have higher usability compared to the \ncorresponding graphical UI versions. These findings suggest that the \nReactGenie system is successful in generating multimodal UIs that \nenhance the user experience, making it easier and more efficient for \nusers to complete tasks. All participants, when using the ReactGenie variant of the app, us", "source": "database"}, "2_197": {"id": "2_197", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ed both typical touch interaction for simple \nnavigation and browsing and touch (optional) + speech interaction \nfor more complex inputs. The combination of graphical and voice \ninterfaces allows users to take advantage of the strengths of each \nmodality, resulting in a more streamlined and enjoyable experience. \n6 DISCUSSION \nIn this section, we will characterize the properties of the multimodal \ninteraction supported by ReactGenie, and discuss the limitations, \nfuture work, safety, and implica", "source": "database"}, "2_198": {"id": "2_198", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "tions of ReactGenie. \n6.1 Properties of Multimodalities in ReactGenie \nWe can characterize the properties of the ReactGenie interactions \nusing the framework proposed by Coutaz et al. [22]. ReactGenie’s \nvoice + (optional) touch actions are implemented using the same \nfunctions used for graphical user interfaces. Therefore, all voice \nand touch actions are equivalent, and almost no actions belong \nto the assignment category, meaning that almost no actions can \nonly be performed using a specific ", "source": "database"}, "2_199": {"id": "2_199", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "modality. Note that the func-\ntions only interact with the state. Commonly minute actions such \nas scrolling will not be included in part of the state, so actions \nlike “Scroll to here” will not be supported by the voice + touch \ncommands. This is intentional, as scrolling is likely more efficient \nusing touch than voice + touch. However, in ReactGenie, devel-\nopers can expose anything as part of the state. In an e-book or a \nmap application, where the current read position/map position is \na cr", "source": "database"}, "2_200": {"id": "2_200", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ucial part of the experience, developers can choose to expose \nthe position as part of the API. The user’s command “scroll to here” \ncan be translated to ReadingPosition.SetPosition(position \n:ReadingPosition.Current()). \nFor redundancy, ReactGenie apps will only accept touch com-\nmands when the microphone button is not activated and will only \naccept voice + touch commands when the microphone button is \nactivated to achieve partial redundancy. For complementarity, Re-\nactGenie primarily support", "source": "database"}, "2_201": {"id": "2_201", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "s voice for actions and touch for deictic \ngestures for reference. After the user clicks the microphone but-\nton, the order of touch and voice does not matter, the user can tap \nfirst then speak, speak first then tap, or tap while speaking. The \nvoice and touch mode will end automatically after receiving no new \nwords or touches for 0.5 seconds. \n6.2 Limitations and Future Work \nReactGenie is the first attempt at integrating multimodal devel-\nopment into the modern declarative GUI development pr", "source": "database"}, "2_202": {"id": "2_202", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ocess. It \nprovides a familiar workflow, allows the reuse of state code and \nUI, and can understand rich multimodal commands. However, it is \nfar from perfect. There are three directions that future work can \nimprove on 1) better voice interfaces, 2) better developer support, \nand 3) support for more modalities. \n6.2.1 Better Voice Interfaces. ReactGenie accepts the user’s voice \nand touch input and generates text and GUI output based on the \nresult. We currently provide text but not voice feedb", "source": "database"}, "2_203": {"id": "2_203", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ack, which \nis easy to change by using a commercial text-to-speech module. \nHowever, a more significant area of improvement is in maintain-\ning natural language context. For example, if the user says, “What  ReactGenie CHI ’24, May 11–16, 2024, Honolulu, HI, USA \nAverage Temporal Demand Frustration Mental Demand Effort Physical Demand Performance020406080100Score\n*TLX Scores,\nthe lower the better\nCondition\nGUI-only\nReactGenie\nTotal020406080100*SUS Scores,\nthe higher the better\nFigure 8: Cognitiv", "source": "database"}, "2_204": {"id": "2_204", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e load (left) and usability (right) of the GUI vs. the ReactGenie multimodal UI. \nis the best pizza restaurant?” and then asks, “What about Chi-\nnese food?”, the system should be able to understand that the \nuser is asking about the best Chinese food instead of any Chi-\nnese food restaurant. Note that ReactGenie can handle some con-\nversations gracefully by using the current UI context as the con-\ntext for the next command. An example would be the user saying, \n“Find me the cheapest hamburger at", "source": "database"}, "2_205": {"id": "2_205", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " McDonald’s” and then asking, \n“Order one of that” (NLPL: Order.GetActiveCart().addItems \n([FoodItem.Current()]) ). ReactGenie would present the food \nitem after the first command, and when the user says the second \ncommand, ReactGenie would know that the user is referring to the \nfood item presented in the GUI. \nAnother way to improve the reliability of the generated inter-\nfaces is to better leverage multimodal commands for disambiguation. \nAs shown in Section 5.1.3, many of our elicited comma", "source": "database"}, "2_206": {"id": "2_206", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nds include \nredundant information from voice and the GUI. Future work can \nleverage this redundancy and provide extra GUI context to the se-\nmantic parser to further push the parser’s accuracy closer to 100%. \nFor example, a potential path is to improve ReactGenie runtime to \nextra GUI context and enhance the neural semantic parser through \nprompt engineering to allow it to process the extra GUI context. \n6.2.2 Better Developer Support. Although ReactGenie provides a \ncustomizable and easy way ", "source": "database"}, "2_207": {"id": "2_207", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "of programming multimodal apps, it can \nstill be improved. One area that we see as a potential improvement \nis to reduce the number of examples necessary and to increase the \neffectiveness of the example parses. The majority of these examples \nare there for teaching the parser how to generate syntactically \ncorrect NLPL code. However, given we have the interpreter, we can \npotentially use it as an example generator to teach the parser how \nto generate syntactically correct NLPL code, similar to ", "source": "database"}, "2_208": {"id": "2_208", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "the method \nused in SEMPRE [13] or Genie [19]. Another route is to fine-tune \nthe Codex model with the NLPL code generated by the interpreter so that the interpreter can generate syntactically correct NLPL code \nwith fewer examples. \nFuture extensions to the ReactGenie framework can also help \ndevelopers identify potential voice commands that the user may \nwant to say. Using ReactGenieFoodOrdering as an example, its API \nonly supports 59% of the commands that we elicited from crowd \nworkers. Som", "source": "database"}, "2_209": {"id": "2_209", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e top categories of unimplemented commands are \nabout delivery time (mentioned in 8 commands), food customization \noptions (8), discount/deal information (7), pickup/delivery support \nof restaurants (6), food types (e.g., vegetarian or vegan) (6), and \ncalorie/health/allergy information (6). If we can implement these \ncommands, we can potentially reduce the number of unsupported \ncommands by more than 50%. Future work can consider embedding \nelicitation studies directly into the app development ", "source": "database"}, "2_210": {"id": "2_210", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "cycle, or the \nframework could record unsupported commands from actual users \nand use this data as feedback to the development team to help \nimprove the system after the initial deployment. \n6.2.3 Support More Modalities. As stated in Section 3.1, ReactGe-\nnie is targeted at gesture and voice interactions, and it is optimized \nfor deictic gestures used for reference. Currently, ReactGenie can \nsupport complex gestures in the regular GUI provided other gesture \ninteraction frameworks, but not sim", "source": "database"}, "2_211": {"id": "2_211", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ultaneous voice + complex ges-\nture interactions. Future frameworks can improve on supporting \nmore diverse gestures and modalities beyond these two categories. \nGestures and modalities other than voice can also be used to \nindicate the actions that the user wants to perform. This can be \nsupported by summarizing what happened using other gestures \nin language and presenting them along with the user’s speech \ncommands. For example, suppose a user says, “Animate this box,” \nand performs a move wh", "source": "database"}, "2_212": {"id": "2_212", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ile rotating and gesturing on the screen. \nIn that case, we can present voice:animatethisbox and gesture \n:rotateandmovefromx1,y1,tox2,y2 to the large language model \nand ask the model to generate NLPL for this animation. (x1,y1 is  CHI ’24, May 11–16, 2024, Honolulu, HI, USA Yang et al. \nthe coordinate of the gesture start coordinate, and the x2,y2 is the \ngesture end coordinate.) \nAnother way gestures can be integrated is through a new class \ncalled Gesture provided to the ReactGenie system to", "source": "database"}, "2_213": {"id": "2_213", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " indicate the \ngesture trajectory that the user performed. For example, if the user \nsays “Deploy soldiers along this path” in a QuickSet-like [21] sys-\ntem, ReactGenie can translate it to Solders.DeployOnPath(path \n:Path.FromGesture(gesture:Gesture.Current())). \nThrough a combination of the aforementioned methods and \nutilization of the gesture recognition/modality understanding com-\nponents of prior multimodal interaction libraries [32, 49], future \nwork can combine the flexibility and express", "source": "database"}, "2_214": {"id": "2_214", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "iveness of ReactGenie \nwith different modalities. \n6.3 Takeaways for Future UI Frameworks \nReactGenie demonstrated a feature-first, rather than interaction-\nfirst, way of implementing multimodal apps. ReactGenie lets devel-\nopers do what they know best: implement the features the users \nwant, and the framework intelligently takes care of users’ diverse \ninteractions using their specific way of making commands. This \nis similar to modern graphical UI frameworks that, rather than \ndirectly passing", "source": "database"}, "2_215": {"id": "2_215", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " the user’s touch point and letting developers im-\nplement the rest, ask developers to provide functions and pages at \na feature level. The graphical UI framework handles how to render \nand convert user touch points and typical gestures to the corre-\nsponding function calls. This was not easy to achieve previously for \nmultimodal interactions. ReactGenie made it possible because of the \npower of LLM’s language-parsing capability and the expressiveness \nof NLPL. Future frameworks can build on thi", "source": "database"}, "2_216": {"id": "2_216", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "s idea to support more \nmodalities and be more adaptive to the user’s interaction context. \nOther researchers in intelligent human interfaces can also take \nadvantage of how ReactGenie integrates LLMs into the human in-\nteraction loop. Rather than asking developers to call LLMs in their \napp, which requires them to understand prompting techniques, \nReactGenie automatically generates an interface to the LLM as a \nprogramming framework. This program-LLM interface provides the \navailable functional", "source": "database"}, "2_217": {"id": "2_217", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ity to the LLM and allows the LLM to call the \ncompound functionality of the program. Other intelligent human \ninterfaces, such as conversational agents or adaptive interfaces, \ncan also learn from this technique. They can automatically gener-\nate an interface from the developer’s code that exposes available \nfeatures and relevant context to LLMs and allows LLMs to use a \nprogramming language (maybe NLPL) to perform their job. \n6.4 Safety and Implications \nReactGenie uses a machine learning mode", "source": "database"}, "2_218": {"id": "2_218", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "l to understand the users’ \ncommands. This may introduce safety issues when the wrong com-\nmand is interpreted and executed. In our evaluation, a pleasant \nfinding is that wrongly parsed commands are either not executable \nor still helpful towards reaching the user’s intended goal. Further \nrisks can be reduced by having a more accurate semantic parser. \nAlso, compared with an end-to-end natural language assistant \nlike ChatGPT7, ReactGenie allows more control over the presented \ninformation and", "source": "database"}, "2_219": {"id": "2_219", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " performed actions. ReactGenie’s framework only \nprocesses and shows information in the developer’s provided state \n7https://openai.com/blog/chatgpt/ code and can reduce hallucinated information. One particular case \nof error in our testing was when the user asked for the delivery \ntime, but because the app does not support delivery time estimation, \nReactGenie returned the delivery fee instead. In this case, the text \nfeedback mechanism can be used to inform the user of the informa-\ntion that i", "source": "database"}, "2_220": {"id": "2_220", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "s returned. In the future, an error correction mechanism \nwould be useful for the user to report the error, and this may allow \nthe developer to fix it. \nFor performed actions, ReactGenie gives text feedback and ren-\nders the related UI elements to ensure the user is aware of the \ncommand being executed, so when there is an error, the user can \neasily identify and recover from it. A design decision we made while \ncreating the three demo apps is not to expose non-recoverable ac-\ntions to voice. F", "source": "database"}, "2_221": {"id": "2_221", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "or example, in the ReactGenieFoodOrdering app, \nthe user can browse items, add items to the cart, and go to the \ncheckout page via voice, but placing the order will only present \nthe checkout page and require the user to click the “Place Order” \nbutton to place the order. This way, the irreversible action is only \ntriggered through the GUI, with little room for error. It should be \nstrongly recommended to developers using ReactGenie to either \nnot expose (via Genie annotations) functions that wo", "source": "database"}, "2_222": {"id": "2_222", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "uld create an \nirreversible effect, such as payment-related or account management \nfunctions, or add a confirmation stage via graphical user interfaces \nor explicit voice commands. \nAnother implication of ReactGenie is the possible negative social \nimplications of noisy multimodal interaction. ReactGenie encour-\nages users to use voice and touch to quickly achieve their goals \nwithout going through multiple UI actions and exploration steps. \nThe benefit of ReactGenie comes from the expressivenes", "source": "database"}, "2_223": {"id": "2_223", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "s of voice \nand touch, but voice interfaces may not always be appropriate. One \npossibility is to explore silent voice interfaces like those presented \nby Denby et al. [26] that can be used in public spaces. \n7 CONCLUSIONS \nCommercial user interfaces have stagnated with the GUI for more \nthan a decade. Although these GUIs work well for communicat-\ning exact information (e.g., from a menu) and binary actions (e.g., \nusing a button), they are not expressive enough to communicate \nand collect rich ", "source": "database"}, "2_224": {"id": "2_224", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "multimodal information, such as the way a waiter \nor waitress can obtain a person’s order from a restaurant menu. \nReactGenie attempts to break that UI stagnation by enabling devel-\nopers to create multimodal UIs that allow for more expressiveness \nthan traditional GUIs, with little additional programming effort. \nReactGenie accomplishes this first by introducing an interaction \nprogramming paradigm where the interaction logic is better sepa-\nrated from user interface implementation and second b", "source": "database"}, "2_225": {"id": "2_225", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "y using a \npowerful natural language understanding module that leverages \nthe capabilities of LLMs to execute code in the interaction logic. In \nthis paper, we demonstrated the easy adoption of the ReactGenie \nframework for developers and tested the expressiveness, usefulness, \nand accuracy of the resulting multimodal apps with end-users. In \nthe future, by introducing developer tools based on frameworks \nlike ReactGenie, and the research on the multimodal interactions \nthese tools enable, we ho", "source": "database"}, "2_226": {"id": "2_226", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "pe to see humans communicating with \ncomputers more expressively and more easily.  ReactGenie CHI ’24, May 11–16, 2024, Honolulu, HI, USA \nACKNOWLEDGMENTS \nWe would like to thank the reviewers for their insightful feedback \nand the participants of our user studies for their invaluable input. \nWe also want to acknowledge Meta Platforms, Inc., the Alfred P. \nSloan Foundation, and the Verdant Foundation for their generous \nfinancial support. We are also grateful to Microsoft for providing \nAzure AI", "source": "database"}, "2_227": {"id": "2_227", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " credits, which have been instrumental in advancing our \nresearch. \nREFERENCES \n[1] [n. d.]. About App Development with UIKit. https://developer.apple.com/docu-\nmentation/uikit/about_app_development_with_uikit . Accessed on 2023-04-05. \n[2] [n. d.]. Describing the UI. https://react.dev/learn/describing-the-ui . Accessed on \n2023-04-05. \n[3] [n. d.]. Introduction to declarative UI. https://docs.flutter.dev/get-started/flutter-\nfor/declarative . Accessed on 2023-04-05. \n[4] [n. d.]. Pinia | The in", "source": "database"}, "2_228": {"id": "2_228", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "tuitive store for Vue.js. https://pinia.vuejs.org/. (Accessed \non 04/05/2023). \n[5] [n. d.]. React Redux | React Redux. https://react-redux.js.org/. (Accessed on \n02/27/2024). \n[6] [n. d.]. Redux - A predictable state container for JavaScript apps. | Redux. \nhttps://redux.js.org/. (Accessed on 04/04/2023). \n[7] [n. d.]. SwiftUI. https://developer.apple.com/xcode/swiftui/. Accessed on 2023-\n04-05. \n[8] 1996. SUS: A 'Quick and Dirty' Usability Scale. In Usability Evaluation In Industry . \nCRC Pres", "source": "database"}, "2_229": {"id": "2_229", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "s, 207–212. https://doi.org/10.1201/9781498710411-35 \n[9] 2009. Human-Computer Interaction. https://doi.org/10.1201/9781420088861 \n[10] 2023. GitHub - facebookarchive/flux: Application Architecture for Building User \nInterfaces. https://github.com/facebookarchive/flux. (Accessed on 04/04/2023). \n[11] Abdullah X. Ali, Meredith Ringel Morris, and Jacob O. Wobbrock. 2019. Crowdlicit: \nA System for Conducting Distributed End-User Elicitation and Identification \nStudies. In Proceedings of the 2019 CH", "source": "database"}, "2_230": {"id": "2_230", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "I Conference on Human Factors in Computing \nSystems. ACM. https://doi.org/10.1145/3290605.3300485 \n[12] Sean Andrist, Dan Bohus, Ashley Feniello, and Nick Saw. 2022. Developing \nMixed Reality Applications with Platform for Situated Intelligence. In 2022 IEEE \nConference on Virtual Reality and 3D User Interfaces Abstracts and Workshops \n(VRW). IEEE. https://doi.org/10.1109/vrw55335.2022.00018 \n[13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic \nParsing on Freebase fro", "source": "database"}, "2_231": {"id": "2_231", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "m Question-Answer Pairs. In Proceedings of the 2013 \nConference on Empirical Methods in Natural Language Processing, EMNLP 2013, \n18-21 October 2013, Grand Hyatt Seattle, Seattle, Washington, USA, A meeting of \nSIGDAT, a Special Interest Group of the ACL. ACL, 1533–1544. https://aclanthology. \norg/D13-1160/ \n[14] Dan Bohus, Sean Andrist, Ashley Feniello, Nick Saw, Mihai Jalobeanu, Patrick \nSweeney, Anne Loomis Thompson, and Eric Horvitz. 2021. Platform for Situated \nIntelligence. https://doi.org", "source": "database"}, "2_232": {"id": "2_232", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "/10.48550/ARXIV.2103.15975 \n[15] Richard A. Bolt. 1980. “Put-that-there”: Voice and gesture at the graphics interface. \nIn Proceedings of the 7th annual conference on Computer graphics and interactive \ntechniques - SIGGRAPH '80. ACM Press. https://doi.org/10.1145/800250.807503 \n[16] Stephen Brewster, Joanna Lumsden, Marek Bell, Malcolm Hall, and Stuart Tasker. \n2003. Multimodal 'eyes-free' interaction techniques for wearable devices. In \nProceedings of the SIGCHI Conference on Human Factors in C", "source": "database"}, "2_233": {"id": "2_233", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "omputing Systems. \nACM. https://doi.org/10.1145/642611.642694 \n[17] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, \nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda \nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, \nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, \nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin \nChess, Jack Clark, Christopher Berner, Sam McCa", "source": "database"}, "2_234": {"id": "2_234", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ndlish, Alec Radford, Ilya \nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. \nhttps://doi.org/10.48550/ARXIV.2005.14165 \n[18] Giovanni Campagna, Rakesh Ramesh, Silei Xu, Michael Fischer, and Monica S. \nLam. 2017. Almond: The Architecture of an Open, Crowdsourced, Privacy-\nPreserving, Programmable Virtual Assistant. In Proceedings of the 26th Interna-\ntional Conference on World Wide Web (WWW ’17). International World Wide Web \nConferences Steering Committee. https://doi.o", "source": "database"}, "2_235": {"id": "2_235", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "rg/10.1145/3038912.3052562 \n[19] Giovanni Campagna, Silei Xu, Mehrad Moradshahi, Richard Socher, and Monica S. \nLam. 2019. Genie: a generator of natural language semantic parsers for virtual \nassistant commands. In Proceedings of the 40th ACM SIGPLAN Conference on \nProgramming Language Design and Implementation. ACM. https://doi.org/10. \n1145/3314221.3314594 \n[20] P. Cohen, D. McGee, S. Oviatt, L. Wu, J. Clow, R. King, S. Julier, and L. Rosenblum. \n1999. Multimodal interaction for 2D and 3D envi", "source": "database"}, "2_236": {"id": "2_236", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ronments [virtual reality]. IEEE \nComputer Graphics and Applications 19, 4 (1999), 10–13. https://doi.org/10.1109/ 38.773958 \n[21] Philip R. Cohen, Michael Johnston, David McGee, Sharon Oviatt, Jay Pittman, Ira \nSmith, Liang Chen, and Josh Clow. 1997. QuickSet: multimodal interaction for \ndistributed applications. In Proceedings of the fifth ACM international conference \non Multimedia - MULTIMEDIA '97. ACM Press. https://doi.org/10.1145/266180. \n266328 \n[22] Joëlle Coutaz, Laurence Nigay, Daniel", "source": "database"}, "2_237": {"id": "2_237", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " Salber, Ann Blandford, Jon May, and \nRichard M. Young. 1995. Four Easy Pieces for Assessing the Usability of Multimodal \nInteraction: The Care Properties. Springer US, 115–120. https://doi.org/10.1007/978-\n1-5041-2896-4_19 \n[23] Deborah Dahl, Paolo Baggia, and Ken Rehor. 2003. Multimodal Architecture and \nInterfaces. Technical Report NOTE-mmi-arch-20031020. W3C. https://www.w3. \norg/TR/mmi-arch/ \n[24] Deborah A. Dahl. 2013. The W3C multimodal architecture and interfaces standard. \nJournal on Mu", "source": "database"}, "2_238": {"id": "2_238", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ltimodal User Interfaces 7, 3 (apr 2013), 171–182. https://doi.org/10. \n1007/s12193-013-0120-5 \n[25] Adrian A. de Freitas, Michael Nebeling, Xiang 'Anthony' Chen, Junrui Yang, \nAkshaye Shreenithi Kirupa Karthikeyan Ranithangam, and Anind K. Dey. 2016. \nSnap-To-It: A User-Inspired Platform for Opportunistic Device Interactions. In \nProceedings of the 2016 CHI Conference on Human Factors in Computing Systems. \nACM. https://doi.org/10.1145/2858036.2858177 \n[26] B. Denby, T. Schultz, K. Honda, T. Hu", "source": "database"}, "2_239": {"id": "2_239", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "eber, J.M. Gilbert, and J.S. Brumberg. 2010. \nSilent speech interfaces. Speech Communication 52, 4 (April 2010), 270–287. \nhttps://doi.org/10.1016/j.specom.2009.08.002 \n[27] Michael H. Fischer, Giovanni Campagna, Euirim Choi, and Monica S. Lam. 2021. \nDIY assistant: a multi-modal end-user programmable virtual assistant. In Proceed-\nings of the 42nd ACM SIGPLAN International Conference on Programming Language \nDesign and Implementation. ACM. https://doi.org/10.1145/3453483.3454046 \n[28] Divyansh ", "source": "database"}, "2_240": {"id": "2_240", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "Garg. 2023. Multi on. https://multion.ai/ \n[29] Andy (Steve) De George and Alex Buck2. 2023. What is windows forms - win-\ndows forms .NET. https://learn.microsoft.com/en-us/dotnet/desktop/winforms/ \noverview/?view=netdesktop-7.0 \n[30] Kilem Li Gwet. 2008. Computing inter-rater reliability and its variance in the \npresence of high agreement. Brit. J. Math. Statist. Psych. 61, 1 (may 2008), 29–48. \nhttps://doi.org/10.1348/000711006x126600 \n[31] Sandra G. Hart. 2006. Nasa-Task Load Index (NASA-TLX)", "source": "database"}, "2_241": {"id": "2_241", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " ; 20 Years Later. Pro-\nceedings of the Human Factors and Ergonomics Society Annual Meeting 50, 9 (oct \n2006), 904–908. https://doi.org/10.1177/154193120605000909 \n[32] Lode Hoste, Bruno Dumas, and Beat Signer. 2011. Mudra: a unified multimodal \ninteraction framework. In Proceedings of the 13th international conference on \nmultimodal interfaces. ACM. https://doi.org/10.1145/2070481.2070500 \n[33] Michael Johnston, John Chen, Patrick Ehlen, Hyuckchul Jung, Jay Lieske, Aarthi \nReddy, Ethan Selfridg", "source": "database"}, "2_242": {"id": "2_242", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "e, Svetlana Stoyanchev, Brant Vasilieff, and Jay Wilpon. 2014. \nMVA: The Multimodal Virtual Assistant. In Proceedings of the 15th Annual Meeting \nof the Special Interest Group on Discourse and Dialogue (SIGDIAL). Association for \nComputational Linguistics. https://doi.org/10.3115/v1/w14-4335 \n[34] Runchang Kang, Anhong Guo, Gierad Laput, Yang Li, and Xiang 'Anthony' Chen. \n2019. Minuet: Multimodal Interaction with an Internet of Things. In Symposium \non Spatial User Interaction. ACM. https://doi", "source": "database"}, "2_243": {"id": "2_243", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": ".org/10.1145/3357251.3357581 \n[35] Glenn E. Krasner and Stephen T. Pope. 1988. A Cookbook for Using the Model-\nView Controller User Interface Paradigm in Smalltalk-80. J. Object Oriented \nProgram. 1, 3 (aug 1988), 26–49. \n[36] Monica S. Lam, Giovanni Campagna, Mehrad Moradshahi, Sina J. Semnani, and \nSilei Xu. 2022. ThingTalk: An Extensible, Executable Representation Language \nfor Task-Oriented Dialogues. https://doi.org/10.48550/ARXIV.2203.12751 \n[37] James A. Landay and Brad A. Myers. 1993. Ex", "source": "database"}, "2_244": {"id": "2_244", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "tending an existing user interface \ntoolkit to support gesture recognition. In INTERACT '93 and CHI '93 conference \ncompanion on Human factors in computing systems - CHI '93. ACM Press. https: \n//doi.org/10.1145/259964.260123 \n[38] Gierad P. Laput, Mira Dontcheva, Gregg Wilensky, Walter Chang, Aseem Agar-\nwala, Jason Linder, and Eytan Adar. 2013. PixelTone: a multimodal interface \nfor image editing. In Proceedings of the SIGCHI Conference on Human Factors in \nComputing Systems. ACM. https://doi.", "source": "database"}, "2_245": {"id": "2_245", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "org/10.1145/2470654.2481301 \n[39] Minkyung Lee and Mark Billinghurst. 2008. A Wizard of Oz study for an AR mul-\ntimodal interface. In Proceedings of the 10th international conference on Multimodal \ninterfaces. ACM. https://doi.org/10.1145/1452392.1452444 \n[40] Toby Jia-Jun Li and Oriana Riva. 2018. Kite: Building Conversational Bots from \nMobile Apps. In Proceedings of the 16th Annual International Conference on Mo-\nbile Systems, Applications, and Services. ACM. https://doi.org/10.1145/3210240. ", "source": "database"}, "2_246": {"id": "2_246", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "\n3210339 \n[41] David L. Martin, Adam J. Cheyer, and Douglas B. Moran. 1999. The open \nagent architecture: A framework for building distributed software systems. Ap-\nplied Artificial Intelligence 13, 1-2 (jan 1999), 91–128. https://doi.org/10.1080/ \n088395199117504 \n[42] Marilyn Rose McGee-Lennon, Andrew Ramsay, David McGookin, and Philip Gray. \n2009. User evaluation of OIDE: a rapid prototyping platform for multimodal \ninteraction. In Proceedings of the 1st ACM SIGCHI symposium on Engineering \ni", "source": "database"}, "2_247": {"id": "2_247", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nteractive computing systems. ACM. https://doi.org/10.1145/1570433.1570476  CHI ’24, May 11–16, 2024, Honolulu, HI, USA Yang et al. \n[43] B.A. Myers, D.A. Giuse, R.B. Dannenberg, B.V. Zanden, D.S. Kosbie, E. Pervin, \nA. Mickish, and P. Marchal. 1990. Garnet: comprehensive support for graphical, \nhighly interactive user interfaces. Computer 23, 11 (Nov. 1990), 71–85. https: \n//doi.org/10.1109/2.60882 \n[44] Brad A. Myers. 1990. A new model for handling input. ACM Transactions on \nInformation Syste", "source": "database"}, "2_248": {"id": "2_248", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ms 8,3(July1990),289–320. https://doi.org/10.1145/98188.98204 \n[45] Brad A. Myers, Dario Giuse, Andrew Mickish, Brad Vander Zanden, David Kosbie, \nRichard McDaniel, James Landay, Matthews Golderg, and Rajan Pathasarathy. \n1994. The garnet user interface development environment. In Conference com-\npanion on Human factors in computing systems - CHI '94. ACM Press. https: \n//doi.org/10.1145/259963.260472 \n[46] Siva Reddy, Mirella Lapata, and Mark Steedman. 2014. Large-scale Semantic \nParsing withou", "source": "database"}, "2_249": {"id": "2_249", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "t Question-Answer Pairs. Transactions of the Association for \nComputational Linguistics 2 (dec 2014), 377–392. https://doi.org/10.1162/tacl_a_ \n00190 \n[47] Ritam Jyoti Sarmah, Yunpeng Ding, Di Wang, Cheuk Yin Phipson Lee, Toby Jia-\nJun Li, and Xiang 'Anthony' Chen. 2020. Geno: A Developer Tool for Authoring \nMultimodal Interaction on Existing Web Applications. In Proceedings of the 33rd \nAnnual ACM Symposium on User Interface Software and Technology. ACM. https: \n//doi.org/10.1145/3379337.341584", "source": "database"}, "2_250": {"id": "2_250", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "8 \n[48] Gianluca Schiavo, Ornella Mich, Michela Ferron, and Nadia Mana. 2020. Trade-\noffs in the design of multimodal interaction for older adults. Behaviour & Infor-\nmation Technology 41, 5 (dec 2020), 1035–1051. https://doi.org/10.1080/0144929x. \n2020.1851768 \n[49] Marcos Serrano, Laurence Nigay, Jean-Yves L. Lawson, Andrew Ramsay, Roderick \nMurray-Smith, and Sebastian Denef. 2008. The openinterface framework: a tool \nfor multimodal interaction.. In CHI '08 Extended Abstracts on Human Factors ", "source": "database"}, "2_251": {"id": "2_251", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "in \nComputing Systems. ACM. https://doi.org/10.1145/1358628.1358881 \n[50] Wai Wa Tang, Kenneth W.K. Lo, Alvin T.S. Chan, Stephen Chan, Hong Va Leong, \nand Grace Ngai. 2011. i*Chameleon: a scalable and extensible framework for mul-\ntimodal interaction. In CHI '11 Extended Abstracts on Human Factors in Computing \nSystems. ACM. https://doi.org/10.1145/1979742.1979703 \n[51] Christiana Tsiourti, João Quintas, Maher Ben-Moussa, Sten Hanke, Niels Alexan-\nder Nijdam, and Dimitri Konstantas. 2017. The Ca", "source": "database"}, "2_252": {"id": "2_252", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "MeLi Framework—A Multi-\nmodal Virtual Companion for Older Adults. In Studies in Computational Intelli-\ngence. Springer International Publishing, 196–217. https://doi.org/10.1007/978-3-\n319-69266-1_10 \n[52] Matthew Turk. 2014. Multimodal interaction: A review. Pattern Recognition \nLetters 36 (jan 2014), 189–195. https://doi.org/10.1016/j.patrec.2013.07.003 \n[53] Bryan Wang, Gang Li, and Yang Li. 2022. Enabling Conversational Interaction \nwith Mobile UI using Large Language Models. https://doi.org", "source": "database"}, "2_253": {"id": "2_253", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "/10.48550/ARXIV. \n2209.08655 \n[54] Silei Xu, Giovanni Campagna, Jian Li, and Monica S. Lam. 2020. Schema2QA: \nHigh-Quality and Low-Cost Q&A Agents for the Structured Web. In Proceed-\nings of the 29th ACM International Conference on Information &amp ; Knowledge \nManagement. ACM. https://doi.org/10.1145/3340531.3411974 \n[55] Jackie (Junrui) Yang, Gaurab Banerjee, Vishesh Gupta, Monica S. Lam, and \nJames A. Landay. 2020. Soundr: Head Position and Orientation Prediction Using \na Microphone Array. In", "source": "database"}, "2_254": {"id": "2_254", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " Proceedings of the 2020 CHI Conference on Human Factors \nin Computing Systems. ACM. https://doi.org/10.1145/3313831.3376427 \n[56] Jackie (Junrui) Yang, Tuochao Chen, Fang Qin, Monica S. Lam, and James A. \nLanday. 2022. HybridTrak: Adding Full-Body Tracking to VR Using an Off-the-\nShelf Webcam. In CHI Conference on Human Factors in Computing Systems. ACM. \nhttps://doi.org/10.1145/3491102.3502045 \n[57] Jackie (Junrui) Yang, Monica S. Lam, and James A. Landay. 2020. DoThisHere: \nMultimodal Interac", "source": "database"}, "2_255": {"id": "2_255", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "tion to Improve Cross-Application Tasks on Mobile Devices. \nIn Proceedings of the 33rd Annual ACM Symposium on User Interface Software and \nTechnology. ACM. https://doi.org/10.1145/3379337.3415841 \n[58] Jackie (Junrui) Yang and James A. Landay. 2019. InfoLED: Augmenting LED \nIndicator Lights for Device Positioning and Communication. In Proceedings of the \n32nd Annual ACM Symposium on User Interface Software and Technology . ACM. \nhttps://doi.org/10.1145/3332165.3347954 \n[59] Chris Zimmerer, Erik", "source": "database"}, "2_256": {"id": "2_256", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " Wolf, Sara Wolf, Martin Fischbach, Jean-Luc Lugrin, and \nMarc Erich Latoschik. 2020. Finally on Par⁈ Multimodal and Unimodal Interaction \nfor Open Creative Design Tasks in Virtual Reality. In Proceedings of the 2020 \nInternational Conference on Multimodal Interaction . ACM. https://doi.org/10. \n1145/3382507.3418850 A GRAMMAR OF NLPL \ntop Fvalue | all_symbol \nall_symbol Findex_symbol (.all_symbol) ? \nindex_symbol Ffunction_call | symbol ([int_literal]) ? \nfunction_call Fsymbol ((parameter_list?)", "source": "database"}, "2_257": {"id": "2_257", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": ")\nparameter_list Fparameter_pair (,parameter_pair)∗ \nparameter_pair Fsymbol:value \nvalue Ftrue | false | int_literal | float_literal | all_symbol | \naccessor | ”string” | [array_value] \naccessor F.value \narray_value Fvalue (,value)∗ \nsymbol F[0 − I\u0016 − /_][0 − I\u0016 − / 0 − 9_]∗ \nint_literal F(+ | -)?[0 − 9]+ \nfloat_literal F(+ | -)?[0 − 9] ∗ .[0 − 9]+ \nB EXAMPLE PARSES \nHere is a code excerpt from the developer-provided example parses \nfor ReactGenieFoodOrdering: \nOrder.Examples = [ \n{ \nuser_uttera", "source": "database"}, "2_258": {"id": "2_258", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "nce: \"What is the total price of my \norder?\", \nexample_parsed: \n\"Order.GetActiveCart().getTotalPrice()\", \n}, \n{ \nuser_utterance: \"Order a burger and two fries.\", \nexample_parsed: \n\"Order.GetActiveCart().addItems(items: [OrderItem.CreateOrderItem(foodItem: FoodItem.GetFoodItem(name: \\\"burger\\\")), \nOrderItem.CreateOrderItem(foodItem: FoodItem.GetFoodItem(name: \\\"fries\\\"), \nquantity: 2)])\", \n}, { \nuser_utterance: \"I would like to place an order for \npick up\", \nexample_parsed: \n\"Order.GetActiveCart(", "source": "database"}, "2_259": {"id": "2_259", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": ").setPickUp(pickup: true)\", \n}, { \nuser_utterance: \"What's the cheapest item in my \norder?\", \nexample_parsed: \n\"Order.GetActiveCart().items.sort(field: .foodItem.price(), ascending: true)[0]\", \n}, { \nuser_utterance: \"What have I ordered last time from \nmcDonalds?\",  ReactGenie \nexample_parsed: \n\"Order.OrderHistory().matching(field: \n.restaurant, value: \nRestaurant.GetRestaurant(name: \\\"mcDonalds\\\"))[0].items\" \n} \n] \nC PROMPT FOR NLPL PARSER \nHere is an example prompt for OpenAI Codex to convert ", "source": "database"}, "2_260": {"id": "2_260", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "the user \ncommand to NLPL and its expected response. Text snippets starting \nwith // are sent to the LLM as part of the prompt. It resembles code \ncomments to help the LLM understand the structure of the prompt. \nTo help the reader understand where different data are generated, \nwe added comments surrounded by <>, which are not part of the \nprompt sent to the LLM. \n// Here are all the functions that we have \n<developer's class skeletons> \nclass Restaurant { \nstring name; \nstring address; \nstring", "source": "database"}, "2_261": {"id": "2_261", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": " cuisine; \nfloat rating; \n// All active restaurants \nstatic Restaurant[] All(); CHI ’24, May 11–16, 2024, Honolulu, HI, USA \n// The current restaurants \nstatic Restaurant Current(); \n// Get a list of foods representing the menu from a \nrestaurant \nFood[] menu; \n// Book reservations on date \nReservation get_reservation(date: DateTime) \n} ... // Examples: \n<example parses> \nuser: get me the best restaurant in Palo Alto \nparsed: Restaurant.all().matching(field: .address, value: \n\"Palo Alto\").sort(f", "source": "database"}, "2_262": {"id": "2_262", "title": "Test PDF", "doc_type": null, "category": null, "tags": [], "content": "ield: .rating, ascending: false) \n... \n// Current User Interaction \n<current user command> \nuser: order the same burger that I ordered at McDonald's \nlast time \nparsed: \n<expected LLM response> \nOrder.Current().addFoods(foods: Order.All().matching(field: \n.restaurant, value: Restaurant.All().matching(field: \n.name, value: \"McDonald's\")).sort(field: .orderTime, \nascending: false)[0].foods) ", "source": "database"}}}